{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd862d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (76.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\iorfi\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (4.49.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\iorfi\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: evaluate in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.4.3)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from evaluate) (0.29.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets>=2.0.0->evaluate) (19.0.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.16)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas->evaluate) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas->evaluate) (2025.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\iorfi\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\iorfi\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\iorfi\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (0.29.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.16)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\iorfi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install transformers\n",
    "%pip install evaluate\n",
    "%pip install scikit-learn\n",
    "%pip install --upgrade datasets\n",
    "import torch\n",
    "from transformers import (\n",
    "    BertTokenizer, BertForSequenceClassification,\n",
    "    T5Tokenizer, T5ForConditionalGeneration,\n",
    "    TrainingArguments, Trainer, Seq2SeqTrainingArguments, Seq2SeqTrainer,\n",
    "    EarlyStoppingCallback, DataCollatorWithPadding, DataCollatorForSeq2Seq\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b546de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CARICAMENTO E PREPARAZIONE DATASET MIGLIORATA ===\n",
      "Distribuzione originale: {0: 183416, 2: 183187, 1: 182764}\n",
      "Label 0: selezionati 20000 campioni\n",
      "Label 1: selezionati 20000 campioni\n",
      "Label 2: selezionati 20000 campioni\n",
      "Dataset bilanciato finale: 60000 campioni\n",
      "Distribuzione finale: {0: 20000, 1: 20000, 2: 20000}\n",
      "Distribuzione originale: {0: 3329, 2: 3278, 1: 3235}\n",
      "Label 0: selezionati 1500 campioni\n",
      "Label 1: selezionati 1500 campioni\n",
      "Label 2: selezionati 1500 campioni\n",
      "Dataset bilanciato finale: 4500 campioni\n",
      "Distribuzione finale: {1: 1500, 2: 1500, 0: 1500}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/root/.cache/huggingface/datasets\"\n",
    "\n",
    "def create_stratified_balanced_dataset(dataset, min_samples_per_label=15000, max_samples_per_label=50000, seed=42):\n",
    "    \"\"\"\n",
    "  Creates a balanced dataset respect to the number of label per class\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    df = dataset.to_pandas()\n",
    "\n",
    "    \n",
    "    label_counts = df['label'].value_counts()\n",
    "    print(f\"Original distribution: {dict(label_counts)}\")\n",
    "\n",
    "    balanced_samples = []\n",
    "\n",
    "    for label in [0, 1, 2]:  # entailment, neutral, contradiction\n",
    "        label_data = df[df['label'] == label].copy()\n",
    "        available_samples = len(label_data)\n",
    "\n",
    "        if available_samples < min_samples_per_label:\n",
    "            print(f\"⚠️  Label {label}: only {available_samples} available samples (min: {min_samples_per_label})\")\n",
    "            selected_samples = label_data\n",
    "        else:\n",
    "            # Uniform sampling based on sequence length\n",
    "            label_data['text_length'] = label_data['premise'].str.len() + label_data['hypothesis'].str.len()\n",
    "\n",
    "            label_data['length_quartile'] = pd.qcut(label_data['text_length'], 4, labels=False, duplicates='drop')\n",
    "\n",
    "            samples_per_quartile = min(max_samples_per_label // 4, available_samples // 4)\n",
    "            quartile_samples = []\n",
    "\n",
    "            for quartile in range(4):\n",
    "                quartile_data = label_data[label_data['length_quartile'] == quartile]\n",
    "                if len(quartile_data) > 0:\n",
    "                    n_samples = min(samples_per_quartile, len(quartile_data))\n",
    "                    sampled = quartile_data.sample(n=n_samples, random_state=seed+quartile)\n",
    "                    quartile_samples.append(sampled)\n",
    "\n",
    "            selected_samples = pd.concat(quartile_samples, ignore_index=True)\n",
    "            selected_samples = selected_samples.drop(['text_length', 'length_quartile'], axis=1)\n",
    "\n",
    "        balanced_samples.append(selected_samples)\n",
    "        print(f\"Label {label}: selezionati {len(selected_samples)} campioni\")\n",
    "\n",
    "    final_df = pd.concat(balanced_samples, ignore_index=True)\n",
    "    final_df = final_df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "    print(f\"Final dataset: {len(final_df)} campioni\")\n",
    "    print(f\"Final distribution: {dict(final_df['label'].value_counts())}\")\n",
    "\n",
    "    return Dataset.from_pandas(final_df)\n",
    "\n",
    "#  LOADING AND BALANCING DATASETS \n",
    "\n",
    "print(\"=== CARICAMENTO E PREPARAZIONE DATASET MIGLIORATA ===\")\n",
    "\n",
    "# Loading dataset\n",
    "esnli_dataset = load_dataset('esnli', trust_remote_code=True)\n",
    "\n",
    "#Dataset balancing\n",
    "esnli_train = create_stratified_balanced_dataset(\n",
    "    esnli_dataset['train'],\n",
    "    min_samples_per_label=5000,\n",
    "    max_samples_per_label=20000,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "esnli_val = create_stratified_balanced_dataset(\n",
    "    esnli_dataset['validation'],\n",
    "    min_samples_per_label=1000,\n",
    "    max_samples_per_label=1500,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab1c7973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73389072e59d49ed9a564feb92bdbb86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb89d74b3f534fd2aa0762d88861b13d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 60000\n",
      "Validation size: 4500\n"
     ]
    }
   ],
   "source": [
    "# SETUP - BERT for Classification + T5 for Explanations\n",
    "\n",
    "# BERT Model for Classification\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=3,\n",
    "    id2label={0: \"Entailment\", 1: \"Neutral\", 2: \"Contradiction\"},\n",
    "    label2id={\"Entailment\": 0, \"Neutral\": 1, \"Contradiction\": 2}\n",
    ").to(device)\n",
    "\n",
    "# BERT Data Preparation Functions\n",
    "def prepare_bert_data(example):\n",
    "    \"\"\"Prepare data for BERT classification\"\"\"\n",
    "    # Combine premise and hypothesis for BERT input\n",
    "    text = f\"{example['premise']} [SEP] {example['hypothesis']}\"\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"label\": example['label']\n",
    "    }\n",
    "\n",
    "# Prepare BERT data\n",
    "bert_train = esnli_train.map(prepare_bert_data)\n",
    "bert_val = esnli_val.map(prepare_bert_data)\n",
    "\n",
    "print(f\"Training size: {len(bert_train)}\")\n",
    "print(f\"Validation size: {len(bert_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05be6bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT Preprocessing\n",
    "def preprocess_bert(batch):\n",
    "    \"\"\"Tokenize data for BERT\"\"\"\n",
    "    return bert_tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3acb1db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9bfbc31e733452497638465ec0ecb9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fca4f76462d48438b7734d8b47785e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply preprocessing\n",
    "bert_train_tokenized = bert_train.map(\n",
    "    preprocess_bert,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "bert_val_tokenized = bert_val.map(\n",
    "    preprocess_bert,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "# Set format for PyTorch\n",
    "bert_train_tokenized.set_format(\"torch\")\n",
    "bert_val_tokenized.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f470e0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT Metrics\n",
    "def compute_bert_metrics(eval_pred):\n",
    "    \"\"\"Metriche dettagliate per analisi completa\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Standard metrics\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1_weighted = f1_score(labels, predictions, average='weighted')\n",
    "    f1_macro = f1_score(labels, predictions, average='macro')\n",
    "\n",
    "    # Class metrics\n",
    "    f1_per_class = f1_score(labels, predictions, average=None)\n",
    "\n",
    "    # Classification report \n",
    "    report = classification_report(labels, predictions, output_dict=True,\n",
    "                                 target_names=['Entailment', 'Neutral', 'Contradiction'])\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_entailment': f1_per_class[0],\n",
    "        'f1_neutral': f1_per_class[1],\n",
    "        'f1_contradiction': f1_per_class[2],\n",
    "        'precision_macro': report['macro avg']['precision'],\n",
    "        'recall_macro': report['macro avg']['recall']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f84eed91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iorfi\\AppData\\Local\\Temp\\ipykernel_1916\\4248213284.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  bert_trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# BERT Training Arguments\n",
    "bert_training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_nli_high_accuracy\",\n",
    "    learning_rate=2e-5,  \n",
    "    per_device_train_batch_size=64,  \n",
    "    per_device_eval_batch_size=64,\n",
    "    gradient_accumulation_steps=2,  \n",
    "    num_train_epochs=5, \n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    max_grad_norm=1.0,\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=3,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_num_workers=4,\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    "    report_to=None,\n",
    ")\n",
    "\n",
    "# BERT Trainer\n",
    "bert_trainer = Trainer(\n",
    "    model=bert_model,\n",
    "    args=bert_training_args,\n",
    "    train_dataset=bert_train_tokenized,\n",
    "    eval_dataset=bert_val_tokenized,\n",
    "    tokenizer=bert_tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=bert_tokenizer),\n",
    "    compute_metrics=compute_bert_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0dc188",
   "metadata": {},
   "source": [
    "TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239f9284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BERT classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: iorfidafabioaz (iorfidafabioaz-universit-degli-studi-di-milano) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\iorfi\\Desktop\\NLP\\wandb\\run-20250627_151347-ndlk5jhd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/iorfidafabioaz-universit-degli-studi-di-milano/huggingface/runs/ndlk5jhd' target=\"_blank\">./bert_nli_high_accuracy</a></strong> to <a href='https://wandb.ai/iorfidafabioaz-universit-degli-studi-di-milano/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/iorfidafabioaz-universit-degli-studi-di-milano/huggingface' target=\"_blank\">https://wandb.ai/iorfidafabioaz-universit-degli-studi-di-milano/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/iorfidafabioaz-universit-degli-studi-di-milano/huggingface/runs/ndlk5jhd' target=\"_blank\">https://wandb.ai/iorfidafabioaz-universit-degli-studi-di-milano/huggingface/runs/ndlk5jhd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='2345' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   3/2345 00:47 < 30:53:46, 0.02 it/s, Epoch 0.00/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Train BERT Model\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining BERT classifier...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mbert_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\trainer.py:2548\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2541\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2542\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2544\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2546\u001b[0m )\n\u001b[0;32m   2547\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2548\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2551\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2554\u001b[0m ):\n\u001b[0;32m   2555\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2556\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\trainer.py:3698\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3695\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3697\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3698\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3700\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3702\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3703\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3704\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\trainer.py:3759\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3757\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[0;32m   3758\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[1;32m-> 3759\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3760\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3761\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1673\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1665\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1666\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1667\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1668\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1669\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1670\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1671\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1673\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1674\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1679\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1680\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1681\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1683\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1685\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1687\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    686\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m         output_attentions,\n\u001b[0;32m    693\u001b[0m     )\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    575\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    584\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 585\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    592\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:515\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    507\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    513\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 515\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    525\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;66;03m# a causal mask in case tgt_len == 1.\u001b[39;00m\n\u001b[0;32m    436\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tgt_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    438\u001b[0m )\n\u001b[1;32m--> 440\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    449\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    450\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_head_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train BERT Model\n",
    "print(\"Training BERT classifier...\")\n",
    "bert_trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc76989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT training completed and model saved!\n"
     ]
    }
   ],
   "source": [
    "# Save BERT model\n",
    "bert_model.save_pretrained(\"BERT_NLI_Modelcpu\")\n",
    "bert_tokenizer.save_pretrained(\"BERT_NLI_TokenizerCPU\")\n",
    "\n",
    "print(\"BERT training completed and model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e57babc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# T5 Model for Explanations\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f19541b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating T5 train dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821b6700dc2d4b16a3cfdc57bcfec352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating T5 validation dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8417b6b4d4054f8f94d56f09e241c028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " T5 train dataset size: 60000\n",
      " T5 val dataset size: 4500\n",
      "\n",
      "Verifying datasets...\n",
      "T5 train sample: {'premise': 'A man attempts to place basketball in hoop while surrounded by opposing teammates.', 'hypothesis': 'man competing in basketball', 'label': 0, 'explanation_1': 'Attempts to place a basketball in a hoop is a way of competing in basketball', 'explanation_2': '', 'explanation_3': '', 'input_text': 'premise: A man attempts to place basketball in hoop while surrounded by opposing teammates. hypothesis: man competing in basketball', 'target_text': 'Attempts to place a basketball in a hoop is a way of competing in basketball'}\n",
      "Input text type: <class 'str'>\n",
      "Target text type: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "#DATA PREPARATION FOR T5 MODEL\n",
    "def prepare_t5_data_corrected(example):\n",
    "    # Maps numeric label into text and vice versa\n",
    "    label_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
    "    label_text = label_map.get(example[\"label\"], \"neutral\")\n",
    "\n",
    "    #Create a suitable input for T%\n",
    "    input_text = f\"premise: {example['premise']} hypothesis: {example['hypothesis']}\"\n",
    "\n",
    "    #We are using the explanations inside e-SNLI dataset\n",
    "    # e-SNLI has explanation_1, explanation_2, explanation_3\n",
    "    # We will use the first explanation available\n",
    "    target_text = None\n",
    "\n",
    "    for explanation_key in ['explanation_1', 'explanation_2', 'explanation_3']:\n",
    "        if explanation_key in example and example[explanation_key] and example[explanation_key].strip():\n",
    "            target_text = example[explanation_key].strip()\n",
    "            break\n",
    "\n",
    "    #If there aren't any explanations, we will create an appropriate one for that pair of sentences\n",
    "    if not target_text:\n",
    "        if label_text == \"entailment\":\n",
    "            target_text = f\"The hypothesis logically follows from the premise. The premise provides sufficient information to conclude that the hypothesis is true. Therefore, the relationship is entailment.\"\n",
    "        elif label_text == \"contradiction\":\n",
    "            target_text = f\"The hypothesis directly contradicts the information in the premise. The premise and hypothesis cannot both be true at the same time. Therefore, the relationship is contradiction.\"\n",
    "        else:  # neutral\n",
    "            target_text = f\"The hypothesis is neither supported nor contradicted by the premise. The premise does not provide enough information to determine if the hypothesis is true or false. Therefore, the relationship is neutral.\"\n",
    "    return {\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": target_text\n",
    "    }\n",
    "\n",
    "#Preparating data for T5 model\n",
    "print(\"Creating T5 train dataset...\")\n",
    "t5_train = esnli_train.map(prepare_t5_data_corrected)\n",
    "\n",
    "print(\"Creating T5 validation dataset...\")\n",
    "t5_val = esnli_val.map(prepare_t5_data_corrected)\n",
    "\n",
    "print(f\" T5 train dataset size: {len(t5_train)}\")\n",
    "print(f\" T5 val dataset size: {len(t5_val)}\")\n",
    "\n",
    "# Verifying data\n",
    "print(\"\\nVerifying datasets...\")\n",
    "sample = t5_train[0]\n",
    "print(f\"T5 train sample: {sample}\")\n",
    "print(f\"Input text type: {type(sample['input_text'])}\")\n",
    "print(f\"Target text type: {type(sample['target_text'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b40c83dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data check:\n",
      "t5_train size: 60000\n",
      "t5_val size: 4500\n",
      "Sample: {'premise': 'A man attempts to place basketball in hoop while surrounded by opposing teammates.', 'hypothesis': 'man competing in basketball', 'label': 0, 'explanation_1': 'Attempts to place a basketball in a hoop is a way of competing in basketball', 'explanation_2': '', 'explanation_3': '', 'input_text': 'premise: A man attempts to place basketball in hoop while surrounded by opposing teammates. hypothesis: man competing in basketball', 'target_text': 'Attempts to place a basketball in a hoop is a way of competing in basketball'}\n"
     ]
    }
   ],
   "source": [
    "#Size checking\n",
    "try:\n",
    "    print(f\"Original data check:\")\n",
    "    print(f\"t5_train size: {len(t5_train)}\")\n",
    "    print(f\"t5_val size: {len(t5_val)}\")\n",
    "    print(f\"Sample: {t5_train[0]}\")\n",
    "except NameError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Make sure t5_train and t5_val datasets are loaded first!\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6b100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function for T5 Model\n",
    "def preprocess_function(examples):\n",
    "    # Tokenizing input sentences\n",
    "    inputs = t5_tokenizer(\n",
    "        examples[\"input_text\"],\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        padding=False, \n",
    "        return_tensors=None \n",
    "    )\n",
    "    # Tokenizza target sentences\n",
    "    with t5_tokenizer.as_target_tokenizer():\n",
    "        targets = t5_tokenizer(\n",
    "            examples[\"target_text\"],\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            return_tensors=None\n",
    "        )\n",
    "    # Assegna le labels\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f026712",
   "metadata": {},
   "outputs": [],
   "source": [
    "del bert_train\n",
    "del bert_val\n",
    "del bert_train_tokenized\n",
    "del bert_val_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ec36d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== APPLYING PREPROCESSING ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa8835a0f2c4eb9807d97281c472edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iorfi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf9fcabac8b48d297cef48934280f8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed premise from train dataset\n",
      "Removed premise from val dataset\n",
      "Removed hypothesis from train dataset\n",
      "Removed hypothesis from val dataset\n",
      "Removed label from train dataset\n",
      "Removed label from val dataset\n",
      "Removed explanation_1 from train dataset\n",
      "Removed explanation_1 from val dataset\n",
      "Removed explanation_2 from train dataset\n",
      "Removed explanation_2 from val dataset\n",
      "Removed explanation_3 from train dataset\n",
      "Removed explanation_3 from val dataset\n",
      "✓ Preprocessing completed successfully\n",
      "Final train columns: ['input_ids', 'attention_mask', 'labels']\n",
      "Final val columns: ['input_ids', 'attention_mask', 'labels']\n",
      "\n",
      "=== VERIFYING PROCESSED DATA ===\n",
      "Train tokenized size: 60000\n",
      "Val tokenized size: 4500\n",
      "Sample keys: ['input_ids', 'attention_mask', 'labels']\n",
      "input_ids: <class 'list'>, length: 28\n",
      "attention_mask: <class 'list'>, length: 28\n",
      "labels: <class 'list'>, length: 23\n",
      "\n",
      "=== CREATING DATA COLLATOR ===\n",
      "\n",
      "=== TESTING DATA COLLATOR ===\n",
      "✓ Data collator test successful\n",
      "Batch contents:\n",
      "  input_ids: shape torch.Size([2, 43]), dtype torch.int64\n",
      "  attention_mask: shape torch.Size([2, 43]), dtype torch.int64\n",
      "  labels: shape torch.Size([2, 23]), dtype torch.int64\n",
      "  decoder_input_ids: shape torch.Size([2, 23]), dtype torch.int64\n",
      "✓ All dimension checks passed\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== APPLYING PREPROCESSING ===\")\n",
    "try:\n",
    "    t5_train_tokenized = t5_train.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        batch_size=64,\n",
    "        remove_columns=['input_text', 'target_text'], \n",
    "    )\n",
    "\n",
    "    t5_val_tokenized = t5_val.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        batch_size=64,\n",
    "        remove_columns=['input_text', 'target_text'],\n",
    "    )\n",
    "\n",
    "    # We will remove all the colums that we are using no more\n",
    "    unwanted_columns = ['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3']\n",
    "    for col in unwanted_columns:\n",
    "        if col in t5_train_tokenized.column_names:\n",
    "            t5_train_tokenized = t5_train_tokenized.remove_columns([col])\n",
    "            print(f\"Removed {col} from train dataset\")\n",
    "\n",
    "        if col in t5_val_tokenized.column_names:\n",
    "            t5_val_tokenized = t5_val_tokenized.remove_columns([col])\n",
    "            print(f\"Removed {col} from val dataset\")\n",
    "\n",
    "    print(\"✓ Preprocessing completed successfully\")\n",
    "    print(f\"Final train columns: {t5_train_tokenized.column_names}\")\n",
    "    print(f\"Final val columns: {t5_val_tokenized.column_names}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"✗ Preprocessing failed: {e}\")\n",
    "    print(\"Debug info:\")\n",
    "    print(f\"Tokenizer type: {type(t5_tokenizer)}\")\n",
    "    print(f\"Available tokenizer methods: {[m for m in dir(t5_tokenizer) if not m.startswith('_')]}\")\n",
    "    raise\n",
    "\n",
    "# 4. verifying data processing\n",
    "print(\"\\n=== VERIFYING PROCESSED DATA ===\")\n",
    "print(f\"Train tokenized size: {len(t5_train_tokenized)}\")\n",
    "print(f\"Val tokenized size: {len(t5_val_tokenized)}\")\n",
    "\n",
    "# Verifying data\n",
    "sample = t5_train_tokenized[0]\n",
    "print(f\"Sample keys: {list(sample.keys())}\")\n",
    "for key, value in sample.items():\n",
    "    print(f\"{key}: {type(value)}, length: {len(value) if hasattr(value, '__len__') else 'N/A'}\")\n",
    "\n",
    "# 5. Data Collator\n",
    "print(\"\\n=== CREATING DATA COLLATOR ===\")\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=t5_tokenizer,\n",
    "    model=t5_model,\n",
    "    label_pad_token_id=-100,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Data collator check\n",
    "print(\"\\n=== TESTING DATA COLLATOR ===\")\n",
    "try:\n",
    "    # Test con un piccolo batch\n",
    "    test_batch = [t5_train_tokenized[i] for i in range(2)]\n",
    "    collated_batch = data_collator(test_batch)\n",
    "\n",
    "    print(\"✓ Data collator test successful\")\n",
    "    print(\"Batch contents:\")\n",
    "    for key, tensor in collated_batch.items():\n",
    "        print(f\"  {key}: shape {tensor.shape}, dtype {tensor.dtype}\")\n",
    "\n",
    "    # Verifica che non ci siano problemi di dimensioni\n",
    "    assert collated_batch[\"input_ids\"].dim() == 2, \"input_ids should be 2D\"\n",
    "    assert collated_batch[\"labels\"].dim() == 2, \"labels should be 2D\"\n",
    "    assert collated_batch[\"input_ids\"].shape[0] == collated_batch[\"labels\"].shape[0], \"Batch size mismatch\"\n",
    "\n",
    "    print(\"✓ All dimension checks passed\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"✗ Data collator test failed: {e}\")\n",
    "    # Debug aggiuntivo\n",
    "    print(\"\\nDebug info:\")\n",
    "    if 'test_batch' in locals():\n",
    "        for i, item in enumerate(test_batch):\n",
    "            print(f\"Item {i}:\")\n",
    "            for key, value in item.items():\n",
    "                print(f\"  {key}: {type(value)}, len: {len(value) if hasattr(value, '__len__') else 'N/A'}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bef820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T5 Metrics\n",
    "def compute_t5_metrics(eval_pred):\n",
    "    \"\"\"Compute metrics for T5 explanation generation\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # Replacing -100 in labels with pad_token_id before decoding\n",
    "    labels = np.where(labels != -100, labels, t5_tokenizer.pad_token_id)\n",
    "\n",
    "    # Decode predictions and labels\n",
    "    decoded_preds = t5_tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = t5_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Simple length-based evaluation\n",
    "    avg_pred_length = np.mean([len(pred.split()) for pred in decoded_preds])\n",
    "    avg_label_length = np.mean([len(label.split()) for label in decoded_labels])\n",
    "\n",
    "    return {\n",
    "        'avg_prediction_length': avg_pred_length,\n",
    "        'avg_target_length': avg_label_length\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13add244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CREATING TRAINER ===\n",
      "✓ Trainer created successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iorfi\\AppData\\Local\\Temp\\ipykernel_1916\\1448336197.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  t5_trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "#Creating Trainer\n",
    "print(\"\\n=== CREATING TRAINER ===\")\n",
    "\n",
    "t5_training_args = Seq2SeqTrainingArguments(\n",
    "output_dir=\"./t5_explanations_high_accuracy\",\n",
    "learning_rate=5e-5,\n",
    "per_device_train_batch_size=32,\n",
    "per_device_eval_batch_size=32,\n",
    "num_train_epochs=4,\n",
    "warmup_ratio=0.1,\n",
    "weight_decay=0.01,\n",
    "max_grad_norm=1.0,\n",
    "logging_steps=100,\n",
    "eval_strategy=\"epoch\",\n",
    "save_strategy=\"epoch\",\n",
    "load_best_model_at_end=True,\n",
    "metric_for_best_model=\"eval_loss\",\n",
    "greater_is_better=False,\n",
    "fp16=torch.cuda.is_available(),\n",
    "predict_with_generate=True,\n",
    "generation_max_length=256,\n",
    "generation_num_beams=4,\n",
    "seed=42,\n",
    "data_seed=42,\n",
    "report_to=None,\n",
    ")\n",
    "        \n",
    "t5_trainer = Seq2SeqTrainer(\n",
    "model=t5_model,\n",
    "args=t5_training_args,\n",
    "train_dataset=t5_train_tokenized,\n",
    "eval_dataset=t5_val_tokenized,\n",
    "tokenizer=t5_tokenizer,\n",
    "data_collator=data_collator,\n",
    "compute_metrics=compute_t5_metrics if 'compute_t5_metrics' in globals() else None,\n",
    "callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be8c8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train T5 Model\n",
    "print(\"Training T5 explanation generator...\")\n",
    "t5_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83a27c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('T5_Explanation_Tokenizer_Finalcpu\\\\tokenizer_config.json',\n",
       " 'T5_Explanation_Tokenizer_Finalcpu\\\\special_tokens_map.json',\n",
       " 'T5_Explanation_Tokenizer_Finalcpu\\\\spiece.model',\n",
       " 'T5_Explanation_Tokenizer_Finalcpu\\\\added_tokens.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save final T5 model\n",
    "t5_model.save_pretrained(\"T5_Explanation_Finalcpu\")\n",
    "t5_tokenizer.save_pretrained(\"T5_Explanation_Tokenizer_Finalcpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dec219",
   "metadata": {},
   "source": [
    "APPLICATION FOR CONTRADDICTION DETECTION IN POLITICAL DEBATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a1396878",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, T5Tokenizer, T5ForConditionalGeneration, BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import pymongo\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import Levenshtein\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict, List, Tuple, Set\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import json\n",
    "class OptimizedPoliticalStatementAnalyzer:\n",
    "    \"\"\"\n",
    "        Optimized analyzer which uses BERT and T5 models respectively for classification and explanation of political sentences NLI relations\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 bert_model_path=\"./bert_nli_Final\", \n",
    "                 bert_tokenizer_path=\"./bert_nli_tokenizer_Final\",\n",
    "                 t5_model_path=\"./T5_Explanation_Final\", \n",
    "                 t5_tokenizer_path=\"./T5_Explanation_Tokenizer_Final\"):\n",
    "       \n",
    "        # Device\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"🚀 Using device: {self.device}\")\n",
    "\n",
    "        print(\"📥 Loading BERT model for classification...\")\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained(bert_tokenizer_path)\n",
    "        self.bert_model = BertForSequenceClassification.from_pretrained(bert_model_path).to(self.device)\n",
    "        \n",
    "        print(\"📥 Loading T5 Model for explanation...\")\n",
    "        self.t5_tokenizer = T5Tokenizer.from_pretrained(t5_tokenizer_path)\n",
    "        self.t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_path).to(self.device)\n",
    "        self.t5_model.eval()\n",
    "        \n",
    "        # Model for sentence embeddings (for semantic similarity)\n",
    "        print(\"📥 Loading SentenceTransformer...\")\n",
    "        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        # Zero-shot classification model (for topic classification)\n",
    "        print(\"📥 Loading zero-shot classificator...\")\n",
    "        self.classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "        \n",
    "        # Database MongoDB\n",
    "        self.client = None\n",
    "        self.db = None\n",
    "        self.collection = None\n",
    "        \n",
    "        # embeddings cache\n",
    "        self.embedding_cache = {}\n",
    "        \n",
    "        # Predefined topics\n",
    "        self.predefined_topics = [\n",
    "            \"economy\", \"healthcare\", \"education\", \"immigration\", \n",
    "            \"environment\", \"security\", \"labor\", \"technology\"\n",
    "        ]\n",
    "        \n",
    "        # NLI label map\n",
    "        self.nli_label_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
    "        \n",
    "        print(\"✅ Inizialitation Completed!\")\n",
    "        \n",
    "    def setup_database(self, mongo_uri, db_name=\"political_analysis\", collection_name=\"statements\"):\n",
    "        \"\"\"Setup connection MongoDB\"\"\"\n",
    "        try:\n",
    "            self.client = MongoClient(mongo_uri)\n",
    "            self.db = self.client[db_name]\n",
    "            self.collection = self.db[collection_name]\n",
    "            \n",
    "            # Crea indici per performance migliori\n",
    "            self.collection.create_index(\"politician\")\n",
    "            self.collection.create_index(\"topic\")\n",
    "            self.collection.create_index([(\"politician\", 1), (\"topic\", 1)])\n",
    "            \n",
    "            print(f\"✅ Connesso al database {db_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Errore connessione database: {e}\")\n",
    "    \n",
    "    def get_sentence_embedding(self, text: str):\n",
    "        \"\"\"Return the semantic sentence embedding, using a cache for efficiency.\"\"\"\n",
    "        if text in self.embedding_cache:\n",
    "            return self.embedding_cache[text]\n",
    "\n",
    "        embedding = self.sentence_model.encode([text])[0]\n",
    "        self.embedding_cache[text] = embedding\n",
    "        return embedding\n",
    "\n",
    "    def find_topic_by_classification(self, statement, custom_topics=None, confidence_threshold=0.35):\n",
    "        \"\"\"\n",
    "        Automatically classifies the topic of a sentence using an iterative mechanism.\n",
    "        If the confidence score falls below the threshold, it removes the predicted topic and tries again.\n",
    "        \"\"\"\n",
    "        topics = custom_topics.copy() if custom_topics else self.predefined_topics.copy()\n",
    "        original_topics = topics.copy()  # Keep the original topics for logging\n",
    "        excluded_topics = []  # Track excluded topics\n",
    "\n",
    "        try:\n",
    "            while len(topics) > 0:\n",
    "                result = self.classifier(statement, topics)\n",
    "                predicted_topic = result['labels'][0]\n",
    "                confidence = result['scores'][0]\n",
    "                \n",
    "                # If confidence exceeds the threshold, return the result\n",
    "                if confidence >= confidence_threshold:\n",
    "                    return {\n",
    "                        'topic': predicted_topic,\n",
    "                        'confidence': confidence,\n",
    "                        'all_scores': dict(zip(result['labels'], result['scores'])),\n",
    "                        'excluded_topics': excluded_topics,\n",
    "                        'iterations': len(excluded_topics) + 1\n",
    "                    }\n",
    "                \n",
    "                # If confidence is below the threshold, remove this topic and try again\n",
    "                excluded_topics.append(predicted_topic)\n",
    "                topics.remove(predicted_topic)\n",
    "                \n",
    "                print(f\"🔄 Confidence {confidence:.3f} below threshold {confidence_threshold}\")\n",
    "                print(f\"   Excluded topic: '{predicted_topic}', {len(topics)} topics remain\")\n",
    "            \n",
    "            # If no topic exceeds the threshold\n",
    "            return {\n",
    "                'topic': 'TOPIC_NOT_FOUND',\n",
    "                'confidence': 0.0,\n",
    "                'all_scores': {},\n",
    "                'excluded_topics': excluded_topics,\n",
    "                'iterations': len(excluded_topics),\n",
    "                'message': f'No topic found with confidence >= {confidence_threshold}'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Topic classification error: {e}\")\n",
    "            return {\n",
    "                'topic': 'unknown', \n",
    "                'confidence': 0.0, \n",
    "                'all_scores': {},\n",
    "                'excluded_topics': [],\n",
    "                'iterations': 0,\n",
    "                'error': str(e)\n",
    "            }\n",
    "\n",
    "    def _classify_nli_relation_bert(self, premise: str, hypothesis: str) -> dict:\n",
    "        \"\"\"\n",
    "        Classify the NLI (Natural Language Inference) relation between a premise and a hypothesis\n",
    "        using the fine‑tuned BERT model.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            bert_input = f\"{premise} [SEP] {hypothesis}\"\n",
    "\n",
    "            # Tokenise with parameters optimised for NLI\n",
    "            inputs = self.bert_tokenizer(\n",
    "                bert_input,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=256,\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                add_special_tokens=True,\n",
    "                return_attention_mask=True,\n",
    "                return_token_type_ids=True\n",
    "            ).to(self.device)\n",
    "\n",
    "            # Forward pass (no gradients needed for inference)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.bert_model(\n",
    "                    input_ids=inputs['input_ids'],\n",
    "                    attention_mask=inputs['attention_mask'],\n",
    "                    token_type_ids=inputs.get('token_type_ids', None),\n",
    "                    output_attentions=False,\n",
    "                    output_hidden_states=False\n",
    "                )\n",
    "\n",
    "                logits = outputs.logits\n",
    "\n",
    "                # Temperature scaling can smooth probabilities if desired\n",
    "                temperature = 1.3\n",
    "                probabilities = F.softmax(logits / temperature, dim=-1)\n",
    "\n",
    "                predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "                confidence = probabilities[0][predicted_class].item()\n",
    "\n",
    "            relation = self.nli_label_map[predicted_class]\n",
    "\n",
    "            return {\n",
    "                'relation': relation,\n",
    "                'confidence': confidence,\n",
    "                'probabilities': {\n",
    "                    'entailment': probabilities[0][0].item(),\n",
    "                    'neutral': probabilities[0][1].item(),\n",
    "                    'contradiction': probabilities[0][2].item()\n",
    "                },\n",
    "                'logits': logits[0].cpu().numpy().tolist()\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error during BERT NLI classification: {e}\")\n",
    "            return {\n",
    "                'relation': 'neutral',\n",
    "                'confidence': 0.0,\n",
    "                'probabilities': {\n",
    "                    'entailment': 0.0,\n",
    "                    'neutral': 1.0,\n",
    "                    'contradiction': 0.0\n",
    "                },\n",
    "                'logits': [0.0, 0.0, 0.0]\n",
    "            }\n",
    "\n",
    "\n",
    "    def _generate_nli_explanation_t5(self, premise: str, hypothesis: str, predicted_label: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a natural‑language explanation for the NLI prediction using the fine‑tuned T5 model.\n",
    "        \"\"\"\n",
    "        input_text = (\n",
    "            \"explain nli as a teacher, make it convincing:: \"\n",
    "            f\"premise: {premise} hypothesis: {hypothesis} label: {predicted_label}\"\n",
    "        )\n",
    "\n",
    "        inputs = self.t5_tokenizer(\n",
    "            input_text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        ).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.t5_model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                min_length=30,\n",
    "                max_new_tokens=150,\n",
    "                num_beams=6,\n",
    "                early_stopping=True,\n",
    "                repetition_penalty=3.0,\n",
    "                no_repeat_ngram_size=4,\n",
    "                length_penalty=1.5,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                top_p=0.92,\n",
    "                temperature=0.3,\n",
    "                forced_eos_token_id=self.t5_tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        explanation = self.t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return explanation\n",
    "\n",
    "    \n",
    "    def _detect_nli_relation_with_explanation(self, premise, hypothesis):\n",
    "        try:\n",
    "            print()\n",
    "            print(f\"🔍 Analisi NLI:\")\n",
    "            print(f\"   Premise: {premise}\")\n",
    "            print(f\"   Hypothesis: {hypothesis}\")\n",
    "            \n",
    "            # 1. Classifiction with BERT\n",
    "            bert_result = self._classify_nli_relation_bert(premise, hypothesis)\n",
    "            \n",
    "            # 2. Generating explanation with T5 Model\n",
    "            predicted_label = bert_result['relation'].title()  # \"Entailment\", \"Neutral\", \"Contradiction\"\n",
    "            t5_explanation = self._generate_nli_explanation_t5(premise, hypothesis, predicted_label)\n",
    "            \n",
    "            # Combining results\n",
    "            result = {\n",
    "                'relation': bert_result['relation'],\n",
    "                'confidence': bert_result['confidence'],\n",
    "                'probabilities': bert_result['probabilities'],\n",
    "                'explanation': t5_explanation,\n",
    "                'source': 'bert_t5_combined'\n",
    "            }\n",
    "            \n",
    "            print(f\"✅ Final Results: {bert_result['relation']} (conf: {bert_result['confidence']:.3f})\")\n",
    "            print(\"probabilities\", result['probabilities'])\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ ERROR in NLI detection: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return {\n",
    "                'relation': 'neutral',\n",
    "                'confidence': 0.0,\n",
    "                'probabilities': {'entailment': 0.0, 'neutral': 1.0, 'contradiction': 0.0},\n",
    "                'explanation': f\"Errore nell'analisi: {str(e)}\",\n",
    "                'source': 'error'\n",
    "            }\n",
    "\n",
    "    def get_politician_statements(self, politician):\n",
    "        \"\"\"Retrieving all the declaration for a specific politician\"\"\"\n",
    "        try:\n",
    "            cursor = self.collection.find({\"politician\": politician})\n",
    "            statements = []\n",
    "\n",
    "            for doc in cursor:\n",
    "                statements.append({\n",
    "                    'statement': doc['statement'],\n",
    "                    'topic': doc.get('topic'),\n",
    "                    'embedding': doc.get('embedding')\n",
    "                })\n",
    "            return statements\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Errore recupero statements: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def find_same_topic_statements_optimized(self, target_statement, politician, \n",
    "                                         topic_hint=None, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Optimized function for topic detection of the target sentence,\n",
    "        comparing it with all the politician's statements to find those on the same topic.\n",
    "        \"\"\"\n",
    "        # Retrieve all statements from the specified politician\n",
    "        politician_statements = self.get_politician_statements(politician)\n",
    "\n",
    "        if not politician_statements:\n",
    "            print(f\"No statements found for politician: {politician}\")\n",
    "            return []\n",
    "\n",
    "        return self._find_by_database_topic(target_statement, politician_statements, topic_hint)\n",
    "\n",
    "    def _find_by_database_topic(self, target_statement, politician_statements, topic_hint=None):\n",
    "        \"\"\"\n",
    "        Finds statements from the same politician that match the topic of the target statement.\n",
    "        If no topic hint is provided, it automatically classifies the target statement's topic.\n",
    "        \"\"\"\n",
    "        # Determine topic to use\n",
    "        if not topic_hint:\n",
    "            topic_info = self.find_topic_by_classification(target_statement)\n",
    "            target_topic = topic_info['topic']\n",
    "        else:\n",
    "            target_topic = topic_hint\n",
    "\n",
    "        same_topic_statements = []\n",
    "\n",
    "        for stmt_data in politician_statements:\n",
    "            stmt_text = stmt_data['statement']\n",
    "            stmt_topic = stmt_data.get('topic')\n",
    "\n",
    "            if stmt_text == target_statement:\n",
    "                continue  # Skip exact match\n",
    "\n",
    "            # Check if topics match\n",
    "            topic_match = False\n",
    "            if stmt_topic and self._topics_match(target_topic, stmt_topic):\n",
    "                topic_match = True\n",
    "            elif not stmt_topic:\n",
    "                # Classify the statement to assign a topic\n",
    "                stmt_topic_info = self.find_topic_by_classification(stmt_text)\n",
    "                if self._topics_match(target_topic, stmt_topic_info['topic']):\n",
    "                    topic_match = True\n",
    "                    stmt_topic = stmt_topic_info['topic']\n",
    "\n",
    "            if topic_match:\n",
    "                # Compute semantic similarity for ranking\n",
    "                similarity = cosine_similarity(\n",
    "                    [self.get_sentence_embedding(target_statement)],\n",
    "                    [self.get_sentence_embedding(stmt_text)]\n",
    "                )[0][0]\n",
    "\n",
    "                same_topic_statements.append({\n",
    "                    'statement': stmt_text,\n",
    "                    'topic': stmt_topic,\n",
    "                    'similarity': float(similarity)\n",
    "                })\n",
    "\n",
    "        return sorted(same_topic_statements, key=lambda x: x['similarity'], reverse=True)\n",
    "\n",
    "    def _topics_match(self, topic1, topic2):\n",
    "        \"\"\"compare topics with fuzzy matching\"\"\"\n",
    "        if not topic1 or not topic2:\n",
    "            return False\n",
    "        \n",
    "        topic1_lower = topic1.lower().strip()\n",
    "        topic2_lower = topic2.lower().strip()\n",
    "        \n",
    "        def topic_similarity(s1, s2, soglia=0.8):\n",
    "            ratio = 1 - Levenshtein.distance(s1, s2) / max(len(s1), len(s2))\n",
    "            return ratio >= soglia, ratio\n",
    "        \n",
    "        # Exact matching\n",
    "        if topic1_lower == topic2_lower:\n",
    "            return True\n",
    "        elif topic_similarity(topic1_lower, topic2_lower)[0]:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def get_available_topics_for_politician(self, politician):\n",
    "        \"\"\"Retrieving all the avaiable topics for a determined politician\"\"\"\n",
    "        try:\n",
    "            pipeline = [\n",
    "                {\"$match\": {\"politician\": politician}},\n",
    "                {\"$group\": {\"_id\": \"$topic\", \"count\": {\"$sum\": 1}}},\n",
    "                {\"$sort\": {\"count\": -1}}\n",
    "            ]\n",
    "            \n",
    "            topics = []\n",
    "            for result in self.collection.aggregate(pipeline):\n",
    "                if result['_id']:\n",
    "                    topics.append({\n",
    "                        'topic': result['_id'],\n",
    "                        'statement_count': result['count']\n",
    "                    })\n",
    "            \n",
    "            return topics\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Errore nel recupero topic: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def add_statement(self, politician, statement, topic=None):\n",
    "        \"\"\"Adds declaration into database with embeddings\"\"\"\n",
    "        try:\n",
    "            # Generating embeddings with sentenceTransformer\n",
    "            embedding = self.get_sentence_embedding(statement).tolist()\n",
    "            \n",
    "            # if there's not a topic, find it through classification\n",
    "            if not topic:\n",
    "                topic_info = self.find_topic_by_classification(statement)\n",
    "                topic = topic_info['topic']\n",
    "            \n",
    "            document = {\n",
    "                \"politician\": politician,\n",
    "                \"statement\": statement,\n",
    "                \"topic\": topic,\n",
    "                \"embedding\": embedding\n",
    "            }\n",
    "            \n",
    "            result = self.collection.insert_one(document)\n",
    "            print(f\"✅ Declaration added with ID: {result.inserted_id}\")\n",
    "            return result.inserted_id\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Errore nell'aggiunta della dichiarazione: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def analyze_statement(self, politician, statement, topic_hint=None, confidence_threshold=0.5):\n",
    "        \"\"\"\n",
    "        Optimized contradiction analysis using BERT + T5\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"\\n🚀 Starting declaration analysis\")\n",
    "            print(f\"👤 Politician: {politician}\")\n",
    "            print(f\"💬 Declaration: '{statement}'\")\n",
    "            print(f\"🏷️ Topic hint: {topic_hint}\")\n",
    "            print(f\"📊 Confidence threshold: {confidence_threshold}\")\n",
    "            \n",
    "            # Find similar statements on the same topic\n",
    "            similar_statements = self.find_same_topic_statements_optimized(\n",
    "                statement, politician, topic_hint, threshold=0.5\n",
    "            )\n",
    "            \n",
    "            print(f\"📊 Similar sentences found: {len(similar_statements)}\")\n",
    "            \n",
    "            if not similar_statements:\n",
    "                if topic_hint:\n",
    "                    return {\n",
    "                        'results': {'contradiction': [], 'entailment': []},\n",
    "                        'stats': {'contradictions': 0, 'entailments': 0},\n",
    "                        'topic_used': topic_hint,\n",
    "                        'search_type': 'exact_topic',\n",
    "                        'same_topic_found': 0,\n",
    "                        'message': f\"No sentence found for topic '{topic_hint}'\"\n",
    "                    }\n",
    "                else:\n",
    "                    topic_info = self.find_topic_by_classification(statement)\n",
    "                    return {\n",
    "                        'results': {'contradiction': [], 'entailment': []},\n",
    "                        'stats': {'contradictions': 0, 'entailments': 0},\n",
    "                        'topic_info': topic_info,\n",
    "                        'search_type': 'auto_detection',\n",
    "                        'same_topic_found': 0,\n",
    "                        'message': f\"No sentence found for topic '{topic_info['topic']}'\"\n",
    "                    }\n",
    "            \n",
    "            # Analyze NLI relations using BERT + T5\n",
    "            contradictions = []\n",
    "            entailments = []\n",
    "            \n",
    "            for stmt_info in similar_statements:\n",
    "                # Use BERT + T5 to detect NLI relations with explanation\n",
    "                nli_result = self._detect_nli_relation_with_explanation(stmt_info['statement'], statement) \n",
    "                \n",
    "                # Filter by confidence\n",
    "                if nli_result['confidence'] >= confidence_threshold:\n",
    "                    entry = {\n",
    "                        'statement': stmt_info['statement'],\n",
    "                        'similarity': stmt_info['similarity'],\n",
    "                        'topic': stmt_info.get('topic'),\n",
    "                        'nli_relation': nli_result['relation'],\n",
    "                        'nli_confidence': nli_result['confidence'],\n",
    "                        'nli_probabilities': nli_result['probabilities'],\n",
    "                        'explanation': nli_result['explanation'],\n",
    "                        'model_source': nli_result['source']\n",
    "                    }\n",
    "                    \n",
    "                    if nli_result['relation'] == 'contradiction':\n",
    "                        contradictions.append(entry)\n",
    "                    elif nli_result['relation'] == 'entailment':\n",
    "                        entailments.append(entry)\n",
    "                else:\n",
    "                    print(f\"⚠️ Confidence too low ({nli_result['confidence']:.3f} < {confidence_threshold})\")\n",
    "            \n",
    "            print(f\"\\n📈 Total contradictions: {len(contradictions)}\")\n",
    "            print(f\"📈 Total entailments: {len(entailments)}\")\n",
    "            \n",
    "            # Prepare response\n",
    "            if topic_hint:\n",
    "                result = {\n",
    "                    'results': {\n",
    "                        'contradiction': contradictions,\n",
    "                        'entailment': entailments\n",
    "                    },\n",
    "                    'stats': {\n",
    "                        'contradictions': len(contradictions),\n",
    "                        'entailments': len(entailments)\n",
    "                    },\n",
    "                    'topic_used': topic_hint,\n",
    "                    'search_type': 'exact_topic',\n",
    "                    'same_topic_found': len(similar_statements),\n",
    "                    'confidence_threshold': confidence_threshold\n",
    "                }\n",
    "            else:\n",
    "                topic_info = self.find_topic_by_classification(statement)\n",
    "                result = {\n",
    "                    'results': {\n",
    "                        'contradiction': contradictions,\n",
    "                        'entailment': entailments\n",
    "                    },\n",
    "                    'stats': {\n",
    "                        'contradictions': len(contradictions),\n",
    "                        'entailments': len(entailments)\n",
    "                    },\n",
    "                    'topic_info': topic_info,\n",
    "                    'search_type': 'auto_detection',\n",
    "                    'same_topic_found': len(similar_statements),\n",
    "                    'confidence_threshold': confidence_threshold\n",
    "                }\n",
    "            \n",
    "            print(f\"🏁 Analysis completed\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in analysis: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return {\n",
    "                'results': {'contradiction': [], 'entailment': []},\n",
    "                'stats': {'contradictions': 0, 'entailments': 0},\n",
    "                'error': str(e)\n",
    "            }\n",
    "\n",
    "    def bulk_import_statements(self, csv_file_path):\n",
    "        \"\"\"Import declarations from CSV file with embedding generation\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file_path)\n",
    "            \n",
    "            documents = []\n",
    "            for _, row in df.iterrows():\n",
    "                # emebedding generation\n",
    "                embedding = self.get_sentence_embedding(row['statement']).tolist()\n",
    "                \n",
    "                # Topic classification if missed\n",
    "                topic = row.get('topic')\n",
    "                if pd.isna(topic) or not topic:\n",
    "                    topic_info = self.find_topic_by_classification(row['statement'])\n",
    "                    topic = topic_info['topic']\n",
    "                \n",
    "                document = {\n",
    "                    \"politician\": row['politician'],\n",
    "                    \"statement\": row['statement'],\n",
    "                    \"topic\": topic,\n",
    "                    \"embedding\": embedding\n",
    "                }\n",
    "                documents.append(document)\n",
    "            \n",
    "            # adding batch\n",
    "            if documents:\n",
    "                result = self.collection.insert_many(documents)\n",
    "                print(f\"✅ Importate {len(result.inserted_ids)} dichiarazioni\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error during the import: {e}\")\n",
    "    \n",
    "    def get_politicians_statistics(self):\n",
    "        \"\"\"Politician statistics from the db\"\"\"\n",
    "        try:\n",
    "            pipeline = [\n",
    "                {\"$group\": {\n",
    "                    \"_id\": \"$politician\",\n",
    "                    \"total_statements\": {\"$sum\": 1},\n",
    "                    \"topics\": {\"$addToSet\": \"$topic\"}\n",
    "                }},\n",
    "                {\"$project\": {\n",
    "                    \"politician\": \"$_id\",\n",
    "                    \"total_statements\": 1,\n",
    "                    \"unique_topics\": {\"$size\": \"$topics\"},\n",
    "                    \"_id\": 0\n",
    "                }}\n",
    "            ]  \n",
    "            return list(self.collection.aggregate(pipeline))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error while computing statistics: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close db connection\"\"\"\n",
    "        if self.client:\n",
    "            self.client.close()\n",
    "            print(\"✅ Connessione database chiusa\")\n",
    "    \n",
    "    def _create_nli_test_cases(self) -> Dict[str, List[Tuple[str, str, str]]]:\n",
    "        \"\"\"\n",
    "        Creates premise-hypothesis pairs organized by NLI test category.\n",
    "        Format: (premise, hypothesis, expected_label)\n",
    "        Labels: 'entailment', 'contradiction', 'neutral'\n",
    "        \"\"\"\n",
    "        test_cases = {\n",
    "            'mutual_exclusion': [\n",
    "                (\"Mario Rossi says he will never increase taxes for self-employed workers\",\n",
    "                \"Mario Rossi will increase taxes for self-employed workers\",\n",
    "                \"contradiction\"),\n",
    "\n",
    "                (\"Giulia Bianchi promises not to cut public healthcare anymore\",\n",
    "                \"Giulia Bianchi will make cuts to public healthcare\",\n",
    "                \"contradiction\"),\n",
    "\n",
    "                (\"Antonio Verdi says he will support universal basic income\",\n",
    "                \"Antonio Verdi opposes universal basic income\",\n",
    "                \"contradiction\"),\n",
    "\n",
    "                (\"Antonio Verdi supports universal basic income\",\n",
    "                \"Antonio Verdi wants to give citizens a monthly payment\",\n",
    "                \"entailment\"),\n",
    "\n",
    "                (\"Maria Neri says Italy will become the European leader in solar energy by 2030\",\n",
    "                \"Maria Neri wants to improve Italy’s energy autonomy\",\n",
    "                \"neutral\"),\n",
    "            ],\n",
    "\n",
    "            'numerical_differences': [\n",
    "                (\"Mario Rossi promises to reduce taxes by 15%\",\n",
    "                \"Mario Rossi will reduce taxes by 20%\",\n",
    "                \"contradiction\"),\n",
    "\n",
    "                (\"Giulia Bianchi will increase teachers' salaries by 10%\",\n",
    "                \"Giulia Bianchi will increase teachers' salaries by 5%\",\n",
    "                \"contradiction\"),\n",
    "\n",
    "                (\"Mario Rossi will invest 5 billion in renewable energy\",\n",
    "                \"Mario Rossi will invest 5 billion in renewable energy\",\n",
    "                \"entailment\"),\n",
    "\n",
    "                (\"Maria Neri will introduce a 15% flat tax\",\n",
    "                \"Maria Neri plans to apply a simplified tax system\",\n",
    "                \"neutral\"),\n",
    "\n",
    "                (\"Antonio Verdi will cut emissions by 30% in 5 years\",\n",
    "                \"Antonio Verdi will cut emissions by 35% in 5 years\",\n",
    "                \"contradiction\"),\n",
    "            ],\n",
    "\n",
    "            'logical_entailment': [\n",
    "                (\"Mario Rossi says he will hire 10,000 new doctors and nurses\",\n",
    "                \"Mario Rossi will increase healthcare personnel\",\n",
    "                \"entailment\"),\n",
    "\n",
    "                (\"Giulia Bianchi will increase teachers' salaries by 10%\",\n",
    "                \"Giulia Bianchi will improve teachers' economic conditions\",\n",
    "                \"entailment\"),\n",
    "\n",
    "                (\"Maria Neri wants to introduce a climate education program in schools\",\n",
    "                \"Maria Neri cares about climate change\",\n",
    "                \"entailment\"),\n",
    "\n",
    "                (\"Luca Gialli will implement digital ID systems\",\n",
    "                \"Luca Gialli will increase funding for public transport\",\n",
    "                \"neutral\"),\n",
    "\n",
    "                (\"Antonio Verdi will install new security cameras in high-risk areas\",\n",
    "                \"Antonio Verdi believes cameras violate privacy\",\n",
    "                \"contradiction\"),\n",
    "            ],\n",
    "\n",
    "            'neutral_relations': [\n",
    "                (\"Mario Rossi will reduce taxes for the middle class\",\n",
    "                \"Mario Rossi will invest in renewable energy\",\n",
    "                \"neutral\"),\n",
    "\n",
    "                (\"Giulia Bianchi will increase teachers' salaries\",\n",
    "                \"Giulia Bianchi will build new schools\",\n",
    "                \"neutral\"),\n",
    "\n",
    "                (\"Antonio Verdi wants to abolish IRAP tax for small businesses\",\n",
    "                \"Antonio Verdi will install security cameras\",\n",
    "                \"neutral\"),\n",
    "\n",
    "                (\"Maria Neri will reduce waiting lists in hospitals\",\n",
    "                \"Maria Neri will plant one million trees\",\n",
    "                \"neutral\"),\n",
    "\n",
    "                (\"Luca Gialli will eliminate the gender pay gap\",\n",
    "                \"Luca Gialli will digitize public administration\",\n",
    "                \"neutral\"),\n",
    "            ],\n",
    "\n",
    "            'specificity_tests': [\n",
    "                (\"Mario Rossi will reduce taxes for the middle class by 15%\",\n",
    "                \"Mario Rossi will reduce taxes\",\n",
    "                \"entailment\"),\n",
    "\n",
    "                (\"Giulia Bianchi will reduce CO2 emissions by 40% by 2030\",\n",
    "                \"Giulia Bianchi will commit to the environment\",\n",
    "                \"entailment\"),\n",
    "\n",
    "                (\"Antonio Verdi will hire more law enforcement officers\",\n",
    "                \"Antonio Verdi will hire exactly 1,200 police officers\",\n",
    "                \"neutral\"),\n",
    "\n",
    "                (\"Maria Neri will improve public healthcare\",\n",
    "                \"Maria Neri will introduce free mental health consultations in all schools\",\n",
    "                \"neutral\"),\n",
    "\n",
    "                (\"Luca Gialli will invest in green urban projects\",\n",
    "                \"Luca Gialli refuses to support ecological plans\",\n",
    "                \"contradiction\"),\n",
    "            ],\n",
    "\n",
    "            'paraphrase_tests': [\n",
    "                (\"Mario Rossi will never increase taxes for self-employed workers\",\n",
    "                \"Mario Rossi will keep taxes stable for freelancers\",\n",
    "                \"entailment\"),\n",
    "\n",
    "                (\"Giulia Bianchi wants to hire 10,000 new teachers\",\n",
    "                \"Giulia Bianchi will increase the teaching staff by ten thousand units\",\n",
    "                \"entailment\"),\n",
    "\n",
    "                (\"Antonio Verdi will fight petty crime in big cities\",\n",
    "                \"Antonio Verdi will address small urban crimes\",\n",
    "                \"entailment\"),\n",
    "\n",
    "                (\"Maria Neri will plant one million trees in Italian cities\",\n",
    "                \"Maria Neri will implement a massive urban forestation campaign\",\n",
    "                \"entailment\"),\n",
    "\n",
    "                (\"Luca Gialli will promote gender equality in companies\",\n",
    "                \"Luca Gialli opposes measures for equal pay\",\n",
    "                \"contradiction\"),\n",
    "            ],\n",
    "\n",
    "            'complex_negation': [\n",
    "                (\"Mario Rossi says that the tax burden is too high and should be reduced\",\n",
    "                \"Mario Rossi thinks that the tax burden is not excessive\",\n",
    "                \"contradiction\"),\n",
    "\n",
    "                (\"Giulia Bianchi states that youth unemployment is the absolute priority\",\n",
    "                \"Giulia Bianchi does not consider youth unemployment a priority\",\n",
    "                \"contradiction\"),\n",
    "\n",
    "                (\"Antonio Verdi will not tolerate citizens' safety being compromised\",\n",
    "                \"Antonio Verdi accepts that there are security problems\",\n",
    "                \"contradiction\"),\n",
    "\n",
    "                (\"Maria Neri denies wanting to privatize healthcare\",\n",
    "                \"Maria Neri supports public healthcare\",\n",
    "                \"entailment\"),\n",
    "\n",
    "                (\"Luca Gialli does not exclude changing the pension reform in the future\",\n",
    "                \"Luca Gialli has confirmed the current pension law is final\",\n",
    "                \"contradiction\"),\n",
    "            ],\n",
    "\n",
    "            'temporal_conditional': [\n",
    "                (\"Mario Rossi will reduce taxes within the next fiscal year\",\n",
    "                \"Mario Rossi will reduce taxes immediately\",\n",
    "                \"neutral\"),\n",
    "\n",
    "                (\"Giulia Bianchi will increase teachers' salaries starting next school year\",\n",
    "                \"Giulia Bianchi has already increased teachers' salaries\",\n",
    "                \"neutral\"),\n",
    "\n",
    "                (\"Antonio Verdi will propose new agreements with countries of origin\",\n",
    "                \"Antonio Verdi has already signed agreements with countries of origin\",\n",
    "                \"neutral\"),\n",
    "\n",
    "                (\"Maria Neri will build 5 new hospitals by 2026\",\n",
    "                \"Maria Neri plans to expand the hospital network in the coming years\",\n",
    "                \"entailment\"),\n",
    "\n",
    "                (\"Luca Gialli will only approve tax reforms if GDP grows\",\n",
    "                \"Luca Gialli will approve tax reforms regardless of economic conditions\",\n",
    "                \"contradiction\"),\n",
    "            ]\n",
    "        }\n",
    "        return test_cases\n",
    "        \n",
    "    def run_nli_evaluation(self, custom_test_cases=None, confidence_threshold=0.5, \n",
    "                            save_results=True, results_file=\"nli_evaluation_results.json\"):\n",
    "            \"\"\"\n",
    "            Runs a comprehensive evaluation of the NLI system on predefined test cases\n",
    "            \n",
    "            Args:\n",
    "                custom_test_cases: Dictionary with custom tests (optional)\n",
    "                confidence_threshold: Minimum confidence threshold to consider a prediction\n",
    "                save_results: Whether to save results to file\n",
    "                results_file: File name to save results\n",
    "                \n",
    "            Returns:\n",
    "                Dict with detailed metrics and analysis by category\n",
    "            \"\"\"\n",
    "            print(\"🚀 Starting comprehensive NLI evaluation...\")\n",
    "            \n",
    "            # Use predefined or custom tests\n",
    "            if custom_test_cases:\n",
    "                test_cases = custom_test_cases\n",
    "            else:\n",
    "                test_cases = self._create_nli_test_cases()\n",
    "            \n",
    "            # Structures to collect results\n",
    "            all_predictions = []\n",
    "            all_true_labels = []\n",
    "            category_results = {}\n",
    "            detailed_results = []\n",
    "            \n",
    "            # Label mapping\n",
    "            label_to_id = {'entailment': 0, 'neutral': 1, 'contradiction': 2}\n",
    "            id_to_label = {0: 'entailment', 1: 'neutral', 2: 'contradiction'}\n",
    "            \n",
    "            print(f\"📊 Tests configured for {len(test_cases)} categories\")\n",
    "            \n",
    "            # Run tests for each category\n",
    "            for category, cases in test_cases.items():\n",
    "                print(f\"\\n🔍 Testing category: {category} ({len(cases)} cases)\")\n",
    "                \n",
    "                category_predictions = []\n",
    "                category_true_labels = []\n",
    "                category_details = []\n",
    "                \n",
    "                for i, (premise, hypothesis, expected_label) in enumerate(cases):\n",
    "                    print(f\"  Test {i+1}/{len(cases)}: {expected_label}\")\n",
    "                    \n",
    "                    try:\n",
    "                        # NLI classification\n",
    "                        result = self._detect_nli_relation_with_explanation(premise, hypothesis)\n",
    "                        \n",
    "                        predicted_label = result['relation']\n",
    "                        confidence = result['confidence']\n",
    "                        \n",
    "                        # Record results\n",
    "                        all_predictions.append(label_to_id[predicted_label])\n",
    "                        all_true_labels.append(label_to_id[expected_label])\n",
    "                        category_predictions.append(label_to_id[predicted_label])\n",
    "                        category_true_labels.append(label_to_id[expected_label])\n",
    "                        \n",
    "                        # Details for analysis\n",
    "                        test_detail = {\n",
    "                            'category': category,\n",
    "                            'premise': premise,\n",
    "                            'hypothesis': hypothesis,\n",
    "                            'expected_label': expected_label,\n",
    "                            'predicted_label': predicted_label,\n",
    "                            'confidence': confidence,\n",
    "                            'probabilities': result['probabilities'],\n",
    "                            'explanation': result['explanation'],\n",
    "                            'correct': predicted_label == expected_label,\n",
    "                            'high_confidence': confidence >= confidence_threshold\n",
    "                        }\n",
    "                        \n",
    "                        detailed_results.append(test_detail)\n",
    "                        category_details.append(test_detail)\n",
    "                        \n",
    "                        # Immediate feedback\n",
    "                        status = \"✅\" if predicted_label == expected_label else \"❌\"\n",
    "                        print(f\"    {status} Pred: {predicted_label} (conf: {confidence:.3f}) | Expected: {expected_label}\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"    ❌ ERROR in test {i+1}: {e}\")\n",
    "                        # Add neutral prediction in case of error\n",
    "                        all_predictions.append(1)  # neutral\n",
    "                        all_true_labels.append(label_to_id[expected_label])\n",
    "                        category_predictions.append(1)\n",
    "                        category_true_labels.append(label_to_id[expected_label])\n",
    "                \n",
    "                # Calculate metrics per category\n",
    "                if category_predictions:\n",
    "                    category_accuracy = accuracy_score(category_true_labels, category_predictions)\n",
    "                    category_precision, category_recall, category_f1, _ = precision_recall_fscore_support(\n",
    "                        category_true_labels, category_predictions, average='weighted', zero_division=0\n",
    "                    )\n",
    "                    \n",
    "                    category_results[category] = {\n",
    "                        'accuracy': float(category_accuracy),\n",
    "                        'precision': float(category_precision),\n",
    "                        'recall': float(category_recall),\n",
    "                        'f1_score': float(category_f1),\n",
    "                        'total_cases': len(cases),\n",
    "                        'correct_predictions': sum(1 for d in category_details if d['correct']),\n",
    "                        'high_confidence_predictions': sum(1 for d in category_details if d['high_confidence']),\n",
    "                        'details': category_details\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"  📈 Category {category}: Accuracy={category_accuracy:.3f}, F1={category_f1:.3f}\")\n",
    "            \n",
    "            # Calculate global metrics\n",
    "            if all_predictions and all_true_labels:\n",
    "                overall_accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "                overall_precision, overall_recall, overall_f1, _ = precision_recall_fscore_support(\n",
    "                    all_true_labels, all_predictions, average='weighted', zero_division=0\n",
    "                )\n",
    "                \n",
    "                # Confusion matrix\n",
    "                conf_matrix = confusion_matrix(all_true_labels, all_predictions)\n",
    "                \n",
    "                # Detailed classification report\n",
    "                class_report = classification_report(\n",
    "                    all_true_labels, all_predictions, \n",
    "                    target_names=['entailment', 'neutral', 'contradiction'],\n",
    "                    output_dict=True, zero_division=0\n",
    "                )\n",
    "                \n",
    "                # Analysis by label\n",
    "                label_analysis = {}\n",
    "                for label_id, label_name in id_to_label.items():\n",
    "                    true_positives = sum(1 for t, p in zip(all_true_labels, all_predictions) if t == label_id and p == label_id)\n",
    "                    false_positives = sum(1 for t, p in zip(all_true_labels, all_predictions) if t != label_id and p == label_id)\n",
    "                    false_negatives = sum(1 for t, p in zip(all_true_labels, all_predictions) if t == label_id and p != label_id)\n",
    "                    \n",
    "                    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "                    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "                    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "                    \n",
    "                    label_analysis[label_name] = {\n",
    "                        'precision': precision,\n",
    "                        'recall': recall,\n",
    "                        'f1_score': f1,\n",
    "                        'true_positives': true_positives,\n",
    "                        'false_positives': false_positives,\n",
    "                        'false_negatives': false_negatives,\n",
    "                        'support': sum(1 for t in all_true_labels if t == label_id)\n",
    "                    }\n",
    "            \n",
    "            # Compile final results\n",
    "            evaluation_results = {\n",
    "                'overall_metrics': {\n",
    "                    'accuracy': float(overall_accuracy),\n",
    "                    'precision': float(overall_precision),\n",
    "                    'recall': float(overall_recall),\n",
    "                    'f1_score': float(overall_f1),\n",
    "                    'total_test_cases': len(all_predictions),\n",
    "                    'confidence_threshold': confidence_threshold\n",
    "                },\n",
    "                'confusion_matrix': conf_matrix.tolist(),\n",
    "                'classification_report': class_report,\n",
    "                'label_analysis': label_analysis,\n",
    "                'category_results': category_results,\n",
    "                'detailed_results': detailed_results,\n",
    "                'error_analysis': self._analyze_errors(detailed_results),\n",
    "            }\n",
    "            \n",
    "            # Save results if requested\n",
    "            if save_results:\n",
    "                try:\n",
    "                    with open(results_file, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(evaluation_results, f, indent=2, ensure_ascii=False)\n",
    "                    print(f\"💾 Results saved to: {results_file}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Save error: {e}\")\n",
    "            \n",
    "            # Print final summary\n",
    "            self._print_evaluation_summary(evaluation_results)\n",
    "            \n",
    "            return evaluation_results\n",
    "        \n",
    "    def _analyze_errors(self, detailed_results):\n",
    "            \"\"\"Analyzes errors to identify common patterns\"\"\"\n",
    "            errors = [r for r in detailed_results if not r['correct']]\n",
    "            \n",
    "            if not errors:\n",
    "                return {'total_errors': 0, 'error_patterns': {}}\n",
    "            \n",
    "            error_patterns = {\n",
    "                'by_category': defaultdict(int),\n",
    "                'by_expected_label': defaultdict(int),\n",
    "                'by_predicted_label': defaultdict(int),\n",
    "                'low_confidence_errors': 0,\n",
    "                'high_confidence_errors': 0,\n",
    "                'confusion_pairs': defaultdict(int)\n",
    "            }\n",
    "            \n",
    "            for error in errors:\n",
    "                error_patterns['by_category'][error['category']] += 1\n",
    "                error_patterns['by_expected_label'][error['expected_label']] += 1\n",
    "                error_patterns['by_predicted_label'][error['predicted_label']] += 1\n",
    "                \n",
    "                if error['confidence'] < 0.5:\n",
    "                    error_patterns['low_confidence_errors'] += 1\n",
    "                else:\n",
    "                    error_patterns['high_confidence_errors'] += 1\n",
    "                \n",
    "                confusion_pair = f\"{error['expected_label']} -> {error['predicted_label']}\"\n",
    "                error_patterns['confusion_pairs'][confusion_pair] += 1\n",
    "            \n",
    "            return {\n",
    "                'total_errors': len(errors),\n",
    "                'error_rate': len(errors) / len(detailed_results),\n",
    "                'error_patterns': dict(error_patterns)\n",
    "            }\n",
    "        \n",
    "    def _print_evaluation_summary(self, results):\n",
    "            \"\"\"Prints a summary of evaluation results\"\"\"\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"📊 NLI EVALUATION SUMMARY\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            overall = results['overall_metrics']\n",
    "            print(f\"🎯 GLOBAL METRICS:\")\n",
    "            print(f\"   • Accuracy:  {overall['accuracy']:.3f}\")\n",
    "            print(f\"   • Precision: {overall['precision']:.3f}\")\n",
    "            print(f\"   • Recall:    {overall['recall']:.3f}\")\n",
    "            print(f\"   • F1-Score:  {overall['f1_score']:.3f}\")\n",
    "            print(f\"   • Total tests: {overall['total_test_cases']}\")\n",
    "            \n",
    "            print(f\"\\n📈 PERFORMANCE BY CATEGORY:\")\n",
    "            for category, metrics in results['category_results'].items():\n",
    "                print(f\"\\n🔸 {category}\")\n",
    "                print(f\"   • Accuracy:  {metrics['accuracy']:.3f}\")\n",
    "                print(f\"   • Precision: {metrics['precision']:.3f}\")\n",
    "                print(f\"   • Recall:    {metrics['recall']:.3f}\")\n",
    "                print(f\"   • F1-Score:  {metrics['f1_score']:.3f}\")\n",
    "                print(f\"   • Total cases: {metrics['total_cases']}\")\n",
    "                print(f\"   • Correct predictions: {metrics['correct_predictions']}\")\n",
    "                print(f\"   • High-confidence predictions: {metrics['high_confidence_predictions']}\")\n",
    "                    \n",
    "            print(f\"\\n🏷️  PERFORMANCE BY LABEL:\")\n",
    "            for label, metrics in results['label_analysis'].items():\n",
    "                print(f\"   • {label:15s}: P={metrics['precision']:.3f}, R={metrics['recall']:.3f}, F1={metrics['f1_score']:.3f} (support: {metrics['support']})\")\n",
    "            \n",
    "            error_analysis = results['error_analysis']\n",
    "            print(f\"\\n❌ ERROR ANALYSIS:\")\n",
    "            print(f\"   • Total errors: {error_analysis['total_errors']} ({error_analysis['error_rate']:.1%})\")\n",
    "            print(f\"   • High confidence errors: {error_analysis['error_patterns']['high_confidence_errors']}\")\n",
    "            print(f\"   • Low confidence errors: {error_analysis['error_patterns']['low_confidence_errors']}\")\n",
    "    \n",
    "            print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475e2be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL VARIABLES (modify them as needed)\n",
    "mongo_uri = \"mongodb://localhost:27017/\"\n",
    "db_name = \"political_analysis\"  # <-- Put here the name of your database\n",
    "collection_name = \"statements\"  # <-- Put here the name of your collection\n",
    "bert_model_path = \"./BERT_NLI_Model\"  # Path to your BERT model\n",
    "bert_tokenizer_path = \"./BERT_NLI_Tokenizer\"  # Path to the BERT tokenizer\n",
    "t5_model_path = \"./T5_Explanation_Final\"  # Path to your T5 model\n",
    "t5_tokenizer_path = \"./T5_Explanation_Tokenizer_Final\"  # Path to the T5 tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6471de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Using device: cpu\n",
      "📥 Loading BERT model for classification...\n",
      "📥 Loading T5 Model for explanation...\n",
      "📥 Loading SentenceTransformer...\n",
      "📥 Loading zero-shot classificator...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inizializzazione completata!\n",
      "✅ Connesso al database political_analysis\n"
     ]
    }
   ],
   "source": [
    "# Only for the first time (import into the db the sentence for testing the application)\n",
    "if __name__ == \"__main__\":\n",
    "    # Inizializza l'analizzatore con i tuoi modelli addestrati\n",
    "    analyzer = OptimizedPoliticalStatementAnalyzer(\n",
    "        bert_model_path=bert_model_path,  \n",
    "        bert_tokenizer_path=bert_tokenizer_path,  \n",
    "        t5_model_path=t5_model_path,  \n",
    "        t5_tokenizer_path=t5_tokenizer_path \n",
    "    )\n",
    "         # Configura database\n",
    "    analyzer.setup_database(\n",
    "        mongo_uri=mongo_uri, \n",
    "        db_name=db_name, #<-- put here the neame of your db\n",
    "        collection_name=collection_name #<-- put here the neame of your collection\n",
    "    )\n",
    "\n",
    "analyzer.bulk_import_statements(\"C:/Users/iorfi/Desktop/NLP/political_statements_sample.csv\") #<-- put here the path of the db csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba61ab41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Using device: cpu\n",
      "📥 Loading BERT model for classification...\n",
      "📥 Loading T5 Model for explanation...\n",
      "📥 Loading SentenceTransformer...\n",
      "📥 Loading zero-shot classificator...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inizialitation Completed!\n",
      "✅ Connesso al database political_analysis\n",
      "=== OPTIMIZED POLITICAL STATEMENT ANALYZER ===\n",
      "Uses semantic embeddings and EXACT search for specified topics\n",
      "\n",
      "\n",
      "📋 Available topics for Mario Rossi:\n",
      "  • healthcare (3 statements)\n",
      "  • economy (3 statements)\n",
      "  • environment (3 statements)\n",
      "  • immigration (1 statements)\n",
      "  • security (1 statements)\n",
      "🔍 Automatically detecting topic and searching contradictions...\n",
      "\n",
      "🚀 Starting declaration analysis\n",
      "👤 Politician: Mario Rossi\n",
      "💬 Declaration: 'We will disinvest from renewable energy'\n",
      "🏷️ Topic hint: None\n",
      "📊 Confidence threshold: 0.5\n",
      "📊 Similar sentences found: 3\n",
      "\n",
      "🔍 Analisi NLI:\n",
      "   Premise: We will invest 5 billion euros in renewable energy over the next three years.\n",
      "   Hypothesis: We will disinvest from renewable energy\n",
      "✅ Final Results: contradiction (conf: 0.504)\n",
      "probabilities {'entailment': 0.33273643255233765, 'neutral': 0.163133442401886, 'contradiction': 0.5041301250457764}\n",
      "\n",
      "🔍 Analisi NLI:\n",
      "   Premise: We will propose a five-year plan for the ecological transition of businesses.\n",
      "   Hypothesis: We will disinvest from renewable energy\n",
      "✅ Final Results: contradiction (conf: 0.570)\n",
      "probabilities {'entailment': 0.1514858901500702, 'neutral': 0.2784387469291687, 'contradiction': 0.5700753927230835}\n",
      "\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Pollution in Italian cities has reached unacceptable levels.\n",
      "   Hypothesis: We will disinvest from renewable energy\n",
      "✅ Final Results: neutral (conf: 0.471)\n",
      "probabilities {'entailment': 0.29124706983566284, 'neutral': 0.4707014560699463, 'contradiction': 0.23805145919322968}\n",
      "⚠️ Confidence too low (0.471 < 0.5)\n",
      "\n",
      "📈 Total contradictions: 2\n",
      "📈 Total entailments: 0\n",
      "🏁 Analysis completed\n",
      "\n",
      "📊 ANALYSIS RESULTS\n",
      "Statement: 'We will disinvest from renewable energy'\n",
      "Automatically detected topic: environment (confidence: 0.36)\n",
      "Similar statements found on same topic: 3\n",
      "Contradictions found: 2\n",
      "\n",
      "⚠️ CONTRADICTIONS FOUND:\n",
      "\n",
      "1. Previous statement:\n",
      "   'We will invest 5 billion euros in renewable energy over the next three years.'\n",
      "   New statement:\n",
      "   'We will disinvest from renewable energy'\n",
      "   Topic: environment\n",
      "   Semantic similarity: 0.62\n",
      "   Logical relation: contradiction\n",
      "   Explanation: The relationship is contradiction. It is not clear if we are going to invest in renewable energy or disinvest from it at the same time.\n",
      "\n",
      "2. Previous statement:\n",
      "   'We will propose a five-year plan for the ecological transition of businesses.'\n",
      "   New statement:\n",
      "   'We will disinvest from renewable energy'\n",
      "   Topic: environment\n",
      "   Semantic similarity: 0.33\n",
      "   Logical relation: contradiction\n",
      "   Explanation: The relationship is contradiction. It is not clear if we will propose a five-year plan for the ecological transition of businesses or whether we will disinvest from renewable energy.\n",
      "\n",
      "👋 Analysis completed!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize the analyzer with your trained models\n",
    "    analyzer = OptimizedPoliticalStatementAnalyzer(\n",
    "        bert_model_path=bert_model_path,\n",
    "        bert_tokenizer_path=bert_tokenizer_path,  \n",
    "        t5_model_path=t5_model_path, \n",
    "        t5_tokenizer_path=t5_tokenizer_path \n",
    "    )\n",
    "    # Setup database\n",
    "    analyzer.setup_database(\n",
    "        mongo_uri=mongo_uri, \n",
    "        db_name=db_name,\n",
    "        collection_name=collection_name\n",
    "    )\n",
    "\n",
    "    print(\"=== OPTIMIZED POLITICAL STATEMENT ANALYZER ===\")\n",
    "    print(\"Uses semantic embeddings and EXACT search for specified topics\\n\")\n",
    "\n",
    "    choice = input(\"Choose option:\\n1. Check statement consistency\\n2. Add statement to database\\n3. Show available topics for politician\\nChoice: \")\n",
    "\n",
    "    if choice == '1':\n",
    "        politician = input(\"Politician name: \")\n",
    "\n",
    "        # Show available topics\n",
    "        available_topics = analyzer.get_available_topics_for_politician(politician)\n",
    "        if available_topics:\n",
    "            print(f\"\\n📋 Available topics for {politician}:\")\n",
    "            for topic_info in available_topics[:10]:  # Show first 10\n",
    "                print(f\"  • {topic_info['topic']} ({topic_info['statement_count']} statements)\")\n",
    "\n",
    "        statement = input(\"\\nStatement: \")\n",
    "        topic_hint = input(\"Statement topic (leave empty for automatic detection): \").strip() or None\n",
    "\n",
    "        if topic_hint:\n",
    "            print(f\"🎯 Searching database for topic: '{topic_hint}'\")\n",
    "        else:\n",
    "            print(\"🔍 Automatically detecting topic and searching contradictions...\")\n",
    "\n",
    "        # Optimized analysis\n",
    "        results = analyzer.analyze_statement(politician, statement, topic_hint)\n",
    "\n",
    "        print(f\"\\n📊 ANALYSIS RESULTS\")\n",
    "        print(f\"Statement: '{statement}'\")\n",
    "\n",
    "        if results.get('search_type') == 'exact_topic':\n",
    "            print(f\"Search type: EXACT for topic '{results['topic_used']}'\")\n",
    "        else:\n",
    "            if 'topic_info' in results:\n",
    "                topic_info = results['topic_info']\n",
    "                print(f\"Automatically detected topic: {topic_info['topic']} (confidence: {topic_info['confidence']:.2f})\")\n",
    "\n",
    "        print(f\"Similar statements found on same topic: {results.get('same_topic_found', 0)}\")\n",
    "        print(f\"Contradictions found: {results['stats']['contradictions']}\")\n",
    "\n",
    "        if 'message' in results:\n",
    "            print(f\"ℹ️ {results['message']}\")\n",
    "\n",
    "        if results['results']['contradiction']:\n",
    "            print(f\"\\n⚠️ CONTRADICTIONS FOUND:\")\n",
    "            for i, contra in enumerate(results['results']['contradiction'], 1):\n",
    "                print(f\"\\n{i}. Previous statement:\")\n",
    "                print(f\"   '{contra['statement']}'\")\n",
    "                print(f\"   New statement:\")\n",
    "                print(f\"   '{statement}'\")\n",
    "                print(f\"   Topic: {contra.get('topic', 'N/A')}\")\n",
    "                print(f\"   Semantic similarity: {contra['similarity']:.2f}\")\n",
    "                print(f\"   Logical relation: {contra.get('nli_relation', 'N/A')}\")\n",
    "                print(f\"   Explanation: {contra['explanation']}\")\n",
    "        else:\n",
    "            print(f\"\\n✅ No contradictions found\")\n",
    "\n",
    "        save_choice = input(f\"\\nDo you want to save this statement in the database? (y/N): \")\n",
    "        if save_choice.lower() == 'y':\n",
    "            if topic_hint:\n",
    "                topic_to_save = topic_hint\n",
    "            else:\n",
    "                topic_to_save = results.get('topic_info', {}).get('topic')\n",
    "            analyzer.add_statement(politician, statement, topic_to_save)\n",
    "            print(\"Statement saved!\")\n",
    "\n",
    "    elif choice == '2':\n",
    "        politician = input(\"Politician name: \")\n",
    "        statement = input(\"Statement: \")\n",
    "        topic = input(\"Topic (leave empty for automatic classification): \").strip() or None\n",
    "\n",
    "        results = analyzer.analyze_statement(politician, statement, topic)\n",
    "        statement_id = analyzer.add_statement(politician, statement, topic)\n",
    "        if topic:\n",
    "            topic_to_save = topic\n",
    "        else:\n",
    "            topic_to_save = results.get('topic_info', {}).get('topic')\n",
    "        if statement_id:\n",
    "            print(\"✅ Statement added successfully!\")\n",
    "        else:\n",
    "            print(\"❌ Error adding the statement\")\n",
    "\n",
    "    elif choice == '3':\n",
    "        politician = input(\"Politician name: \")\n",
    "        topics = analyzer.get_available_topics_for_politician(politician)\n",
    "        if topics:\n",
    "            print(f\"\\n📋 Available topics for {politician}:\")\n",
    "            for topic_info in topics:\n",
    "                print(f\"  • {topic_info['topic']} ({topic_info['statement_count']} statements)\")\n",
    "        else:\n",
    "            print(f\"❌ No topics found for {politician}\")\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid choice\")\n",
    "\n",
    "    print(f\"\\n👋 Analysis completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f8728e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Using device: cpu\n",
      "📥 Loading BERT model for classification...\n",
      "📥 Loading T5 Model for explanation...\n",
      "📥 Loading SentenceTransformer...\n",
      "📥 Loading zero-shot classificator...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inizialitation Completed!\n",
      "✅ Connesso al database political_analysis\n",
      "🚀 Starting comprehensive NLI evaluation...\n",
      "📊 Tests configured for 8 categories\n",
      "\n",
      "🔍 Testing category: mutual_exclusion (5 cases)\n",
      "  Test 1/5: contradiction\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Mario Rossi says he will never increase taxes for self-employed workers\n",
      "   Hypothesis: Mario Rossi will increase taxes for self-employed workers\n",
      "✅ Final Results: contradiction (conf: 0.453)\n",
      "probabilities {'entailment': 0.23972633481025696, 'neutral': 0.30754706263542175, 'contradiction': 0.4527266323566437}\n",
      "    ✅ Pred: contradiction (conf: 0.453) | Expected: contradiction\n",
      "  Test 2/5: contradiction\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Giulia Bianchi promises not to cut public healthcare anymore\n",
      "   Hypothesis: Giulia Bianchi will make cuts to public healthcare\n",
      "✅ Final Results: entailment (conf: 0.594)\n",
      "probabilities {'entailment': 0.5943519473075867, 'neutral': 0.2288503497838974, 'contradiction': 0.17679770290851593}\n",
      "    ❌ Pred: entailment (conf: 0.594) | Expected: contradiction\n",
      "  Test 3/5: contradiction\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Antonio Verdi says he will support universal basic income\n",
      "   Hypothesis: Antonio Verdi opposes universal basic income\n",
      "✅ Final Results: neutral (conf: 0.484)\n",
      "probabilities {'entailment': 0.21392902731895447, 'neutral': 0.4844921827316284, 'contradiction': 0.30157873034477234}\n",
      "    ❌ Pred: neutral (conf: 0.484) | Expected: contradiction\n",
      "  Test 4/5: entailment\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Antonio Verdi supports universal basic income\n",
      "   Hypothesis: Antonio Verdi wants to give citizens a monthly payment\n",
      "✅ Final Results: neutral (conf: 0.678)\n",
      "probabilities {'entailment': 0.12059438228607178, 'neutral': 0.6782091856002808, 'contradiction': 0.20119649171829224}\n",
      "    ❌ Pred: neutral (conf: 0.678) | Expected: entailment\n",
      "  Test 5/5: neutral\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Maria Neri says Italy will become the European leader in solar energy by 2030\n",
      "   Hypothesis: Maria Neri wants to improve Italy’s energy autonomy\n",
      "✅ Final Results: neutral (conf: 0.646)\n",
      "probabilities {'entailment': 0.175227090716362, 'neutral': 0.6462360620498657, 'contradiction': 0.1785368174314499}\n",
      "    ✅ Pred: neutral (conf: 0.646) | Expected: neutral\n",
      "  📈 Category mutual_exclusion: Accuracy=0.400, F1=0.400\n",
      "\n",
      "🔍 Testing category: numerical_differences (5 cases)\n",
      "  Test 1/5: contradiction\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Mario Rossi promises to reduce taxes by 15%\n",
      "   Hypothesis: Mario Rossi will reduce taxes by 20%\n",
      "✅ Final Results: contradiction (conf: 0.352)\n",
      "probabilities {'entailment': 0.319181889295578, 'neutral': 0.32840394973754883, 'contradiction': 0.3524141311645508}\n",
      "    ✅ Pred: contradiction (conf: 0.352) | Expected: contradiction\n",
      "  Test 2/5: contradiction\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Giulia Bianchi will increase teachers' salaries by 10%\n",
      "   Hypothesis: Giulia Bianchi will increase teachers' salaries by 5%\n",
      "✅ Final Results: contradiction (conf: 0.520)\n",
      "probabilities {'entailment': 0.2552586495876312, 'neutral': 0.22439466416835785, 'contradiction': 0.5203466415405273}\n",
      "    ✅ Pred: contradiction (conf: 0.520) | Expected: contradiction\n",
      "  Test 3/5: entailment\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Mario Rossi will invest 5 billion in renewable energy\n",
      "   Hypothesis: Mario Rossi will invest 5 billion in renewable energy\n",
      "✅ Final Results: entailment (conf: 0.736)\n",
      "probabilities {'entailment': 0.735905110836029, 'neutral': 0.1840057373046875, 'contradiction': 0.08008918911218643}\n",
      "    ✅ Pred: entailment (conf: 0.736) | Expected: entailment\n",
      "  Test 4/5: neutral\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Maria Neri will introduce a 15% flat tax\n",
      "   Hypothesis: Maria Neri plans to apply a simplified tax system\n",
      "✅ Final Results: neutral (conf: 0.502)\n",
      "probabilities {'entailment': 0.15517567098140717, 'neutral': 0.5019778609275818, 'contradiction': 0.34284651279449463}\n",
      "    ✅ Pred: neutral (conf: 0.502) | Expected: neutral\n",
      "  Test 5/5: contradiction\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Antonio Verdi will cut emissions by 30% in 5 years\n",
      "   Hypothesis: Antonio Verdi will cut emissions by 35% in 5 years\n",
      "✅ Final Results: contradiction (conf: 0.486)\n",
      "probabilities {'entailment': 0.22995541989803314, 'neutral': 0.28393852710723877, 'contradiction': 0.4861060678958893}\n",
      "    ✅ Pred: contradiction (conf: 0.486) | Expected: contradiction\n",
      "  📈 Category numerical_differences: Accuracy=1.000, F1=1.000\n",
      "\n",
      "🔍 Testing category: logical_entailment (5 cases)\n",
      "  Test 1/5: entailment\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Mario Rossi says he will hire 10,000 new doctors and nurses\n",
      "   Hypothesis: Mario Rossi will increase healthcare personnel\n",
      "✅ Final Results: neutral (conf: 0.495)\n",
      "probabilities {'entailment': 0.3891577422618866, 'neutral': 0.4945688545703888, 'contradiction': 0.11627332866191864}\n",
      "    ❌ Pred: neutral (conf: 0.495) | Expected: entailment\n",
      "  Test 2/5: entailment\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Giulia Bianchi will increase teachers' salaries by 10%\n",
      "   Hypothesis: Giulia Bianchi will improve teachers' economic conditions\n",
      "✅ Final Results: neutral (conf: 0.709)\n",
      "probabilities {'entailment': 0.1667013168334961, 'neutral': 0.7088183164596558, 'contradiction': 0.12448034435510635}\n",
      "    ❌ Pred: neutral (conf: 0.709) | Expected: entailment\n",
      "  Test 3/5: entailment\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Maria Neri wants to introduce a climate education program in schools\n",
      "   Hypothesis: Maria Neri cares about climate change\n",
      "✅ Final Results: neutral (conf: 0.754)\n",
      "probabilities {'entailment': 0.17635536193847656, 'neutral': 0.7540253400802612, 'contradiction': 0.06961934268474579}\n",
      "    ❌ Pred: neutral (conf: 0.754) | Expected: entailment\n",
      "  Test 4/5: neutral\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Luca Gialli will implement digital ID systems\n",
      "   Hypothesis: Luca Gialli will increase funding for public transport\n",
      "✅ Final Results: neutral (conf: 0.615)\n",
      "probabilities {'entailment': 0.20723934471607208, 'neutral': 0.6147112846374512, 'contradiction': 0.17804932594299316}\n",
      "    ✅ Pred: neutral (conf: 0.615) | Expected: neutral\n",
      "  Test 5/5: contradiction\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Antonio Verdi will install new security cameras in high-risk areas\n",
      "   Hypothesis: Antonio Verdi believes cameras violate privacy\n",
      "✅ Final Results: neutral (conf: 0.650)\n",
      "probabilities {'entailment': 0.16517895460128784, 'neutral': 0.6497970819473267, 'contradiction': 0.18502399325370789}\n",
      "    ❌ Pred: neutral (conf: 0.650) | Expected: contradiction\n",
      "  📈 Category logical_entailment: Accuracy=0.200, F1=0.067\n",
      "\n",
      "🔍 Testing category: neutral_relations (5 cases)\n",
      "  Test 1/5: neutral\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Mario Rossi will reduce taxes for the middle class\n",
      "   Hypothesis: Mario Rossi will invest in renewable energy\n",
      "✅ Final Results: neutral (conf: 0.589)\n",
      "probabilities {'entailment': 0.1584887057542801, 'neutral': 0.5888731479644775, 'contradiction': 0.25263816118240356}\n",
      "    ✅ Pred: neutral (conf: 0.589) | Expected: neutral\n",
      "  Test 2/5: neutral\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Giulia Bianchi will increase teachers' salaries\n",
      "   Hypothesis: Giulia Bianchi will build new schools\n",
      "✅ Final Results: neutral (conf: 0.674)\n",
      "probabilities {'entailment': 0.09039873629808426, 'neutral': 0.6738721132278442, 'contradiction': 0.2357291579246521}\n",
      "    ✅ Pred: neutral (conf: 0.674) | Expected: neutral\n",
      "  Test 3/5: neutral\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Antonio Verdi wants to abolish IRAP tax for small businesses\n",
      "   Hypothesis: Antonio Verdi will install security cameras\n",
      "✅ Final Results: neutral (conf: 0.527)\n",
      "probabilities {'entailment': 0.06994437426328659, 'neutral': 0.5272013545036316, 'contradiction': 0.40285423398017883}\n",
      "    ✅ Pred: neutral (conf: 0.527) | Expected: neutral\n",
      "  Test 4/5: neutral\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Maria Neri will reduce waiting lists in hospitals\n",
      "   Hypothesis: Maria Neri will plant one million trees\n",
      "✅ Final Results: neutral (conf: 0.553)\n",
      "probabilities {'entailment': 0.05262890085577965, 'neutral': 0.552871584892273, 'contradiction': 0.3944995403289795}\n",
      "    ✅ Pred: neutral (conf: 0.553) | Expected: neutral\n",
      "  Test 5/5: neutral\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Luca Gialli will eliminate the gender pay gap\n",
      "   Hypothesis: Luca Gialli will digitize public administration\n",
      "✅ Final Results: neutral (conf: 0.588)\n",
      "probabilities {'entailment': 0.18656739592552185, 'neutral': 0.587547779083252, 'contradiction': 0.22588486969470978}\n",
      "    ✅ Pred: neutral (conf: 0.588) | Expected: neutral\n",
      "  📈 Category neutral_relations: Accuracy=1.000, F1=1.000\n",
      "\n",
      "🔍 Testing category: specificity_tests (5 cases)\n",
      "  Test 1/5: entailment\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Mario Rossi will reduce taxes for the middle class by 15%\n",
      "   Hypothesis: Mario Rossi will reduce taxes\n",
      "✅ Final Results: neutral (conf: 0.447)\n",
      "probabilities {'entailment': 0.31732669472694397, 'neutral': 0.44670888781547546, 'contradiction': 0.2359643429517746}\n",
      "    ❌ Pred: neutral (conf: 0.447) | Expected: entailment\n",
      "  Test 2/5: entailment\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Giulia Bianchi will reduce CO2 emissions by 40% by 2030\n",
      "   Hypothesis: Giulia Bianchi will commit to the environment\n",
      "✅ Final Results: entailment (conf: 0.523)\n",
      "probabilities {'entailment': 0.5230653285980225, 'neutral': 0.3938908874988556, 'contradiction': 0.08304382115602493}\n",
      "    ✅ Pred: entailment (conf: 0.523) | Expected: entailment\n",
      "  Test 3/5: neutral\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Antonio Verdi will hire more law enforcement officers\n",
      "   Hypothesis: Antonio Verdi will hire exactly 1,200 police officers\n",
      "✅ Final Results: neutral (conf: 0.552)\n",
      "probabilities {'entailment': 0.1779060661792755, 'neutral': 0.5522637367248535, 'contradiction': 0.2698301076889038}\n",
      "    ✅ Pred: neutral (conf: 0.552) | Expected: neutral\n",
      "  Test 4/5: neutral\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Maria Neri will improve public healthcare\n",
      "   Hypothesis: Maria Neri will introduce free mental health consultations in all schools\n",
      "✅ Final Results: neutral (conf: 0.556)\n",
      "probabilities {'entailment': 0.15669624507427216, 'neutral': 0.5561684966087341, 'contradiction': 0.2871352732181549}\n",
      "    ✅ Pred: neutral (conf: 0.556) | Expected: neutral\n",
      "  Test 5/5: contradiction\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Luca Gialli will invest in green urban projects\n",
      "   Hypothesis: Luca Gialli refuses to support ecological plans\n",
      "✅ Final Results: contradiction (conf: 0.614)\n",
      "probabilities {'entailment': 0.07439456880092621, 'neutral': 0.3116118907928467, 'contradiction': 0.6139935255050659}\n",
      "    ✅ Pred: contradiction (conf: 0.614) | Expected: contradiction\n",
      "  📈 Category specificity_tests: Accuracy=0.800, F1=0.787\n",
      "\n",
      "🔍 Testing category: paraphrase_tests (5 cases)\n",
      "  Test 1/5: entailment\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Mario Rossi will never increase taxes for self-employed workers\n",
      "   Hypothesis: Mario Rossi will keep taxes stable for freelancers\n",
      "✅ Final Results: contradiction (conf: 0.631)\n",
      "probabilities {'entailment': 0.08144071698188782, 'neutral': 0.2876712381839752, 'contradiction': 0.630888044834137}\n",
      "    ❌ Pred: contradiction (conf: 0.631) | Expected: entailment\n",
      "  Test 2/5: entailment\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Giulia Bianchi wants to hire 10,000 new teachers\n",
      "   Hypothesis: Giulia Bianchi will increase the teaching staff by ten thousand units\n",
      "✅ Final Results: neutral (conf: 0.620)\n",
      "probabilities {'entailment': 0.12151684612035751, 'neutral': 0.6200280785560608, 'contradiction': 0.2584550976753235}\n",
      "    ❌ Pred: neutral (conf: 0.620) | Expected: entailment\n",
      "  Test 3/5: entailment\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Antonio Verdi will fight petty crime in big cities\n",
      "   Hypothesis: Antonio Verdi will address small urban crimes\n",
      "✅ Final Results: entailment (conf: 0.526)\n",
      "probabilities {'entailment': 0.525853157043457, 'neutral': 0.3436591625213623, 'contradiction': 0.13048778474330902}\n",
      "    ✅ Pred: entailment (conf: 0.526) | Expected: entailment\n",
      "  Test 4/5: entailment\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Maria Neri will plant one million trees in Italian cities\n",
      "   Hypothesis: Maria Neri will implement a massive urban forestation campaign\n",
      "✅ Final Results: neutral (conf: 0.684)\n",
      "probabilities {'entailment': 0.08052170276641846, 'neutral': 0.6843959093093872, 'contradiction': 0.23508237302303314}\n",
      "    ❌ Pred: neutral (conf: 0.684) | Expected: entailment\n",
      "  Test 5/5: contradiction\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Luca Gialli will promote gender equality in companies\n",
      "   Hypothesis: Luca Gialli opposes measures for equal pay\n",
      "✅ Final Results: neutral (conf: 0.558)\n",
      "probabilities {'entailment': 0.167860746383667, 'neutral': 0.5576825737953186, 'contradiction': 0.2744567394256592}\n",
      "    ❌ Pred: neutral (conf: 0.558) | Expected: contradiction\n",
      "  📈 Category paraphrase_tests: Accuracy=0.200, F1=0.320\n",
      "\n",
      "🔍 Testing category: complex_negation (5 cases)\n",
      "  Test 1/5: contradiction\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Mario Rossi says that the tax burden is too high and should be reduced\n",
      "   Hypothesis: Mario Rossi thinks that the tax burden is not excessive\n",
      "✅ Final Results: neutral (conf: 0.544)\n",
      "probabilities {'entailment': 0.22417958080768585, 'neutral': 0.543786346912384, 'contradiction': 0.23203407227993011}\n",
      "    ❌ Pred: neutral (conf: 0.544) | Expected: contradiction\n",
      "  Test 2/5: contradiction\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Giulia Bianchi states that youth unemployment is the absolute priority\n",
      "   Hypothesis: Giulia Bianchi does not consider youth unemployment a priority\n",
      "✅ Final Results: contradiction (conf: 0.552)\n",
      "probabilities {'entailment': 0.17449477314949036, 'neutral': 0.27303171157836914, 'contradiction': 0.5524734854698181}\n",
      "    ✅ Pred: contradiction (conf: 0.552) | Expected: contradiction\n",
      "  Test 3/5: contradiction\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Antonio Verdi will not tolerate citizens' safety being compromised\n",
      "   Hypothesis: Antonio Verdi accepts that there are security problems\n",
      "✅ Final Results: neutral (conf: 0.625)\n",
      "probabilities {'entailment': 0.2871237099170685, 'neutral': 0.6245254278182983, 'contradiction': 0.0883508250117302}\n",
      "    ❌ Pred: neutral (conf: 0.625) | Expected: contradiction\n",
      "  Test 4/5: entailment\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Maria Neri denies wanting to privatize healthcare\n",
      "   Hypothesis: Maria Neri supports public healthcare\n",
      "✅ Final Results: neutral (conf: 0.442)\n",
      "probabilities {'entailment': 0.35791176557540894, 'neutral': 0.4417245090007782, 'contradiction': 0.20036372542381287}\n",
      "    ❌ Pred: neutral (conf: 0.442) | Expected: entailment\n",
      "  Test 5/5: contradiction\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Luca Gialli does not exclude changing the pension reform in the future\n",
      "   Hypothesis: Luca Gialli has confirmed the current pension law is final\n",
      "✅ Final Results: neutral (conf: 0.653)\n",
      "probabilities {'entailment': 0.1740318387746811, 'neutral': 0.6526030898094177, 'contradiction': 0.1733650267124176}\n",
      "    ❌ Pred: neutral (conf: 0.653) | Expected: contradiction\n",
      "  📈 Category complex_negation: Accuracy=0.200, F1=0.320\n",
      "\n",
      "🔍 Testing category: temporal_conditional (5 cases)\n",
      "  Test 1/5: neutral\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Mario Rossi will reduce taxes within the next fiscal year\n",
      "   Hypothesis: Mario Rossi will reduce taxes immediately\n",
      "✅ Final Results: neutral (conf: 0.449)\n",
      "probabilities {'entailment': 0.3442002832889557, 'neutral': 0.4488253593444824, 'contradiction': 0.20697437226772308}\n",
      "    ✅ Pred: neutral (conf: 0.449) | Expected: neutral\n",
      "  Test 2/5: neutral\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Giulia Bianchi will increase teachers' salaries starting next school year\n",
      "   Hypothesis: Giulia Bianchi has already increased teachers' salaries\n",
      "✅ Final Results: neutral (conf: 0.472)\n",
      "probabilities {'entailment': 0.3348304331302643, 'neutral': 0.4716492295265198, 'contradiction': 0.19352030754089355}\n",
      "    ✅ Pred: neutral (conf: 0.472) | Expected: neutral\n",
      "  Test 3/5: neutral\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Antonio Verdi will propose new agreements with countries of origin\n",
      "   Hypothesis: Antonio Verdi has already signed agreements with countries of origin\n",
      "✅ Final Results: neutral (conf: 0.634)\n",
      "probabilities {'entailment': 0.25818535685539246, 'neutral': 0.633723795413971, 'contradiction': 0.10809091478586197}\n",
      "    ✅ Pred: neutral (conf: 0.634) | Expected: neutral\n",
      "  Test 4/5: entailment\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Maria Neri will build 5 new hospitals by 2026\n",
      "   Hypothesis: Maria Neri plans to expand the hospital network in the coming years\n",
      "✅ Final Results: neutral (conf: 0.598)\n",
      "probabilities {'entailment': 0.24184592068195343, 'neutral': 0.5979880094528198, 'contradiction': 0.16016604006290436}\n",
      "    ❌ Pred: neutral (conf: 0.598) | Expected: entailment\n",
      "  Test 5/5: contradiction\n",
      "🔍 Analisi NLI:\n",
      "   Premise: Luca Gialli will only approve tax reforms if GDP grows\n",
      "   Hypothesis: Luca Gialli will approve tax reforms regardless of economic conditions\n",
      "✅ Final Results: neutral (conf: 0.400)\n",
      "probabilities {'entailment': 0.3441801071166992, 'neutral': 0.4000709354877472, 'contradiction': 0.25574901700019836}\n",
      "    ❌ Pred: neutral (conf: 0.400) | Expected: contradiction\n",
      "  📈 Category temporal_conditional: Accuracy=0.600, F1=0.450\n",
      "💾 Results saved to: nli_evaluation_results.json\n",
      "\n",
      "================================================================================\n",
      "📊 NLI EVALUATION SUMMARY\n",
      "================================================================================\n",
      "🎯 GLOBAL METRICS:\n",
      "   • Accuracy:  0.550\n",
      "   • Precision: 0.689\n",
      "   • Recall:    0.550\n",
      "   • F1-Score:  0.516\n",
      "   • Total tests: 40\n",
      "\n",
      "📈 PERFORMANCE BY CATEGORY:\n",
      "\n",
      "🔸 mutual_exclusion\n",
      "   • Accuracy:  0.400\n",
      "   • Precision: 0.667\n",
      "   • Recall:    0.400\n",
      "   • F1-Score:  0.400\n",
      "   • Total cases: 5\n",
      "   • Correct predictions: 2\n",
      "   • High-confidence predictions: 3\n",
      "\n",
      "🔸 numerical_differences\n",
      "   • Accuracy:  1.000\n",
      "   • Precision: 1.000\n",
      "   • Recall:    1.000\n",
      "   • F1-Score:  1.000\n",
      "   • Total cases: 5\n",
      "   • Correct predictions: 5\n",
      "   • High-confidence predictions: 3\n",
      "\n",
      "🔸 logical_entailment\n",
      "   • Accuracy:  0.200\n",
      "   • Precision: 0.040\n",
      "   • Recall:    0.200\n",
      "   • F1-Score:  0.067\n",
      "   • Total cases: 5\n",
      "   • Correct predictions: 1\n",
      "   • High-confidence predictions: 4\n",
      "\n",
      "🔸 neutral_relations\n",
      "   • Accuracy:  1.000\n",
      "   • Precision: 1.000\n",
      "   • Recall:    1.000\n",
      "   • F1-Score:  1.000\n",
      "   • Total cases: 5\n",
      "   • Correct predictions: 5\n",
      "   • High-confidence predictions: 5\n",
      "\n",
      "🔸 specificity_tests\n",
      "   • Accuracy:  0.800\n",
      "   • Precision: 0.867\n",
      "   • Recall:    0.800\n",
      "   • F1-Score:  0.787\n",
      "   • Total cases: 5\n",
      "   • Correct predictions: 4\n",
      "   • High-confidence predictions: 4\n",
      "\n",
      "🔸 paraphrase_tests\n",
      "   • Accuracy:  0.200\n",
      "   • Precision: 0.800\n",
      "   • Recall:    0.200\n",
      "   • F1-Score:  0.320\n",
      "   • Total cases: 5\n",
      "   • Correct predictions: 1\n",
      "   • High-confidence predictions: 5\n",
      "\n",
      "🔸 complex_negation\n",
      "   • Accuracy:  0.200\n",
      "   • Precision: 0.800\n",
      "   • Recall:    0.200\n",
      "   • F1-Score:  0.320\n",
      "   • Total cases: 5\n",
      "   • Correct predictions: 1\n",
      "   • High-confidence predictions: 4\n",
      "\n",
      "🔸 temporal_conditional\n",
      "   • Accuracy:  0.600\n",
      "   • Precision: 0.360\n",
      "   • Recall:    0.600\n",
      "   • F1-Score:  0.450\n",
      "   • Total cases: 5\n",
      "   • Correct predictions: 3\n",
      "   • High-confidence predictions: 2\n",
      "\n",
      "🏷️  PERFORMANCE BY LABEL:\n",
      "   • entailment     : P=0.750, R=0.231, F1=0.353 (support: 13)\n",
      "   • neutral        : P=0.448, R=1.000, F1=0.619 (support: 13)\n",
      "   • contradiction  : P=0.857, R=0.429, F1=0.571 (support: 14)\n",
      "\n",
      "❌ ERROR ANALYSIS:\n",
      "   • Total errors: 18 (45.0%)\n",
      "   • High confidence errors: 13\n",
      "   • Low confidence errors: 5\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Inizializza l'analizzatore con i tuoi modelli addestrati\n",
    "    analyzer = OptimizedPoliticalStatementAnalyzer(\n",
    "        bert_model_path=\"./BERT_NLI_Model\",  # Percorso del tuo modello BERT\n",
    "        bert_tokenizer_path=\"./BERT_NLI_Tokenizer\",  # Percorso del tokenizer BERT\n",
    "        t5_model_path=\"./T5_Explanation_Model\",  # Percorso del tuo modello T5\n",
    "        t5_tokenizer_path=\"./T5_Explanation_Tokenizer\"  # Percorso del tokenizer T5\n",
    "    )\n",
    "         # Configura database\n",
    "    analyzer.setup_database(\n",
    "        mongo_uri=\"mongodb://localhost:27017/\", \n",
    "        db_name=\"political_analysis\"\n",
    "    )\n",
    "results = analyzer.run_nli_evaluation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
